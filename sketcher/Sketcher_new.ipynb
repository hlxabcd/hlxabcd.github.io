{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sketcher.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hlxabcd/hlxabcd.github.io/blob/master/sketcher/Sketcher_new.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "zlx6-LFL_jbi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This file contains a subset of the quick draw classes. I choose around 100 classes from the dataset. "
      ]
    },
    {
      "metadata": {
        "id": "6H3ATAdp_URp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Get the Class names "
      ]
    },
    {
      "metadata": {
        "id": "XXv-xzU1sd88",
        "colab_type": "code",
        "outputId": "f79a90bf-7dad-4495-e082-bfb7b26b693a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        }
      },
      "cell_type": "code",
      "source": [
        "!wget 'https://raw.githubusercontent.com/hlxabcd/hlxabcd.github.io/master/sketcher/class_names.txt'"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-03-20 11:26:13--  https://raw.githubusercontent.com/hlxabcd/hlxabcd.github.io/master/sketcher/class_names.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2790 (2.7K) [text/plain]\n",
            "Saving to: ‘class_names.txt.2’\n",
            "\n",
            "\rclass_names.txt.2     0%[                    ]       0  --.-KB/s               \rclass_names.txt.2   100%[===================>]   2.72K  --.-KB/s    in 0s      \n",
            "\n",
            "2019-03-20 11:26:13 (36.4 MB/s) - ‘class_names.txt.2’ saved [2790/2790]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4GL_TdMffD6-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Read the classes names "
      ]
    },
    {
      "metadata": {
        "id": "eP-OxOx5sy0b",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "f = open(\"class_names.txt\",\"r\")\n",
        "# And for reading use\n",
        "classes = f.readlines()\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lTE6D3uxtMc5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "classes = [c.replace('\\n','').replace(' ','_') for c in classes]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5NDfBHVjACAt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Download the Dataset "
      ]
    },
    {
      "metadata": {
        "id": "7MC_PUS-fKjH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Loop over the classes and download the currospondent data"
      ]
    },
    {
      "metadata": {
        "id": "rdSUnpL0u22Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e9bd2cdc-5568-459b-a91c-98f3d8025350"
      },
      "cell_type": "code",
      "source": [
        "!mkdir data"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘data’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "22DPhL5FtWcQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "import os\n",
        "def download():\n",
        "  \n",
        "  base = 'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/'\n",
        "  for c in classes:\n",
        "    cls_url = c.replace('_', '%20')\n",
        "    if not os.path.exists('data/' + c + '.npy'):\n",
        "        path = base+cls_url+'.npy'\n",
        "        print(path)\n",
        "        urllib.request.urlretrieve(path, 'data/'+c+'.npy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O5jF6TXXu-Bu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6191
        },
        "outputId": "6b8eff07-2e61-45f7-920d-73ae54fc5ff2"
      },
      "cell_type": "code",
      "source": [
        "download() "
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/alarm%20clock.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/ambulance.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/angel.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/animal%20migration.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/ant.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/anvil.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/apple.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/arm.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/asparagus.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/axe.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/backpack.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/banana.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/bandage.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/barn.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/baseball.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/baseball%20bat.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/basket.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/basketball.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/bat.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/bathtub.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/beach.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/bear.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/beard.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/bed.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/bee.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/belt.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/bench.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/bicycle.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/binoculars.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/bird.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/birthday%20cake.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/blackberry.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/blueberry.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/book.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/boomerang.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/bottlecap.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/bowtie.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/bracelet.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/brain.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/bread.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/bridge.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/broccoli.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/broom.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/bucket.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/bulldozer.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/bus.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/bush.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/butterfly.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/cactus.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/cake.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/calculator.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/calendar.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/camel.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/camera.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/camouflage.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/campfire.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/candle.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/cannon.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/canoe.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/car.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/carrot.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/castle.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/cat.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/ceiling%20fan.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/cello.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/cell%20phone.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/chair.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/chandelier.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/church.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/circle.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/clarinet.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/clock.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/cloud.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/coffee%20cup.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/compass.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/computer.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/cookie.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/cooler.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/couch.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/cow.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/crab.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/crayon.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/crocodile.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/crown.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/cruise%20ship.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/cup.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/diamond.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/dishwasher.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/diving%20board.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/dog.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/dolphin.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/donut.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/door.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/dragon.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/dresser.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/drill.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/drums.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/duck.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/dumbbell.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/ear.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/elbow.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/elephant.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/envelope.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/eraser.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/eye.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/eyeglasses.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/face.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/fan.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/feather.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/fence.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/finger.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/fire%20hydrant.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/fireplace.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/firetruck.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/fish.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/flamingo.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/flashlight.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/flip%20flops.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/floor%20lamp.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/flower.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/flying%20saucer.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/foot.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/fork.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/frog.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/frying%20pan.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/garden.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/garden%20hose.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/giraffe.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/goatee.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/golf%20club.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/grapes.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/grass.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/guitar.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/hamburger.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/hammer.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/hand.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/harp.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/hat.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/headphones.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/hedgehog.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/helicopter.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/helmet.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/hexagon.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/hockey%20puck.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/hockey%20stick.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/horse.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/hospital.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/hot%20air%20balloon.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/hot%20dog.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/hot%20tub.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/hourglass.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/house.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/house%20plant.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/hurricane.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/ice%20cream.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/jacket.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/jail.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/kangaroo.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/key.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/keyboard.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/knee.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/knife.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/ladder.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/lantern.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/laptop.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/leaf.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/leg.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/light%20bulb.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/lighter.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/lighthouse.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/lightning.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/line.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/lion.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/lipstick.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/lobster.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/lollipop.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/mailbox.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/map.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/marker.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/matches.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/megaphone.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/mermaid.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/microphone.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/microwave.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/monkey.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/moon.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/mosquito.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/motorbike.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/mountain.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/mouse.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/moustache.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/mouth.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/mug.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/mushroom.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/nail.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/necklace.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/nose.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/ocean.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/octagon.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/octopus.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/onion.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/oven.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/owl.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/paintbrush.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/paint%20can.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/palm%20tree.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/panda.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/pants.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/paper%20clip.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/parachute.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/parrot.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/passport.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/peanut.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/pear.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/peas.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/pencil.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/penguin.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/piano.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/pickup%20truck.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/picture%20frame.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/pig.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/pillow.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/pineapple.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/pizza.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/pliers.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/police%20car.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/pond.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/pool.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/popsicle.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/postcard.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/potato.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/power%20outlet.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/purse.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/rabbit.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/raccoon.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/radio.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/rain.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/rainbow.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/rake.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/remote%20control.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/rhinoceros.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/rifle.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/river.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/roller%20coaster.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/rollerskates.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/sailboat.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/sandwich.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/saw.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/saxophone.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/school%20bus.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/scissors.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/scorpion.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/screwdriver.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/sea%20turtle.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/see%20saw.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/shark.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/sheep.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/shoe.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/shorts.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/shovel.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/sink.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/skateboard.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/skull.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/skyscraper.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/sleeping%20bag.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/smiley%20face.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/snail.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/snake.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/snorkel.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/snowflake.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/snowman.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/soccer%20ball.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/sock.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/speedboat.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/spider.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/spoon.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/spreadsheet.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/square.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/squiggle.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/squirrel.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/stairs.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/star.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/steak.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/stereo.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/stethoscope.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/stitches.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/stop%20sign.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/stove.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/strawberry.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/streetlight.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/string%20bean.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/submarine.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/suitcase.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/sun.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/swan.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/sweater.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/swing%20set.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/sword.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/syringe.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/table.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/teapot.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/teddy-bear.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/telephone.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/television.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/tennis%20racquet.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/tent.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/The%20Eiffel%20Tower.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/The%20Great%20Wall%20of%20China.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/The%20Mona%20Lisa.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/tiger.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/toaster.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/toe.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/toilet.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/tooth.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/toothbrush.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/toothpaste.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/tornado.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/tractor.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/traffic%20light.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/train.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/tree.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/triangle.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/trombone.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/truck.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/trumpet.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/t-shirt.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/umbrella.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/underwear.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/van.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/vase.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/violin.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/washing%20machine.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/watermelon.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/waterslide.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/whale.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/wheel.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/windmill.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/wine%20bottle.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/wine%20glass.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/wristwatch.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/yoga.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/zebra.npy\n",
            "https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/zigzag.npy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uEdnbBVXAI-X",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Imports "
      ]
    },
    {
      "metadata": {
        "id": "J2FYrPgOKh6t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow import keras \n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rxgNFjNM5sWj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 分割npy库提高加载效率"
      ]
    },
    {
      "metadata": {
        "id": "SisxobWa54B-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def split_dataset(root,target,max_items_per_class= 1000 ):\n",
        "     all_files = glob.glob(os.path.join(root, '*.npy'))\n",
        "     for idx, file in enumerate(all_files):\n",
        "        print(\"process npy:\",idx,file)\n",
        "        data = np.load(file)\n",
        "        # npy长度\n",
        "        dataset_len = len(data)\n",
        "        start = 0\n",
        "        end = max_items_per_class\n",
        "        num = 1\n",
        "        fileName = file.split('/')[1].split('.')[0]\n",
        "        # 按max_items_per_class分割npy文件\n",
        "        while (start < dataset_len):\n",
        "            resultData = data[start: end, :]\n",
        "            # 分割后的文件命名\n",
        "            targetFile = os.path.join(target, fileName+'_'+str(num)+'.npy');\n",
        "            if not os.path.exists(targetFile):\n",
        "              np.save(targetFile, resultData)\n",
        "            num+=1\n",
        "            start+=max_items_per_class\n",
        "            end+=max_items_per_class\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CbXnUx_z90Iw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# # 分割npy\n",
        "# !mkdir data_process2\n",
        "# split_dataset('data','data_process2')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6o30ipBPAQ5Y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Load the Data "
      ]
    },
    {
      "metadata": {
        "id": "UBq3GXEKAYuO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Each class contains different number samples of arrays stored as .npy format. Since we have some memory limitations we only load 5000 images per class.  "
      ]
    },
    {
      "metadata": {
        "id": "yEw6qLRwi8P3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# data = np.load('data/aircraft_carrier.npy')\n",
        "# print(data.shape)\n",
        "# max_items_per_class = 10\n",
        "# data = data[0: max_items_per_class, :]\n",
        "# print(data)\n",
        "# labels = np.full(data.shape[0], 1)\n",
        "\n",
        "# x = np.empty([0, 784])\n",
        "# x = np.concatenate((x, data), axis=0)\n",
        "\n",
        "# image_size=28\n",
        "\n",
        "# print(x)\n",
        "\n",
        "# x = x.reshape(x.shape[0], image_size, image_size, 1)\n",
        "# print(x)\n",
        "# # x_test = x_test.reshape(x_test.shape[0], image_size, image_size, 1).astype('float32')/255.0\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6HEIgQNHYQnl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_data(root, vfold_ratio=0.2, max_items_per_class= 2000 ):\n",
        "    all_files = glob.glob(os.path.join(root, '*.npy'))\n",
        "\n",
        "    #initialize variables \n",
        "    x = np.empty([0, 784])\n",
        "    y = np.empty([0])\n",
        "    class_names = []\n",
        "\n",
        "    #load each data file \n",
        "    for idx, file in enumerate(all_files):\n",
        "        print(idx,file)\n",
        "        data = np.load(file)\n",
        "        data = data[0: max_items_per_class, :]\n",
        "        labels = np.full(data.shape[0], idx)\n",
        "\n",
        "        x = np.concatenate((x, data), axis=0)\n",
        "        y = np.append(y, labels)\n",
        "\n",
        "        class_name, ext = os.path.splitext(os.path.basename(file))\n",
        "        class_names.append(class_name)\n",
        "\n",
        "    data = None\n",
        "    labels = None\n",
        "    \n",
        "    #randomize the dataset \n",
        "    permutation = np.random.permutation(y.shape[0])\n",
        "    x = x[permutation, :]\n",
        "    y = y[permutation]\n",
        "\n",
        "    #separate into training and testing \n",
        "    vfold_size = int(x.shape[0]/100*(vfold_ratio*100))\n",
        "\n",
        "    x_test = x[0:vfold_size, :]\n",
        "    y_test = y[0:vfold_size]\n",
        "\n",
        "    x_train = x[vfold_size:x.shape[0], :]\n",
        "    y_train = y[vfold_size:y.shape[0]]\n",
        "    return x_train, y_train, x_test, y_test, class_names\n",
        "\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "\n",
        "  \n",
        "def load_data_process(root, data_process_num, vfold_ratio=0.2):\n",
        "    all_files = glob.glob(os.path.join(root, '*_'+str(data_process_num)+'.npy'))\n",
        "\n",
        "    #initialize variables \n",
        "    max_items_per_class = 1000\n",
        "    x = np.zeros([max_items_per_class * len(all_files), 784])\n",
        "#     x = np.empty([0, 784])\n",
        "    y = np.empty([0])\n",
        "    class_names = []\n",
        "    print(\"allfile:\",len(all_files))\n",
        "    #load each data file \n",
        "    for idx, file in enumerate(all_files):\n",
        "#         print(idx,file)\n",
        "        t = time.time()\n",
        "        data = np.load(file)\n",
        "#         a = time.time()-t\n",
        "#         print('a:',a)\n",
        "        labels = np.full(data.shape[0], idx)\n",
        "        x[idx * max_items_per_class:(idx + 1) * max_items_per_class] = data\n",
        "#         x = np.concatenate((x, data), axis=0)\n",
        "        y = np.append(y, labels)\n",
        "#         b = time.time()-t\n",
        "#         print('b:',b)\n",
        "        class_name, ext = os.path.splitext(os.path.basename(file))\n",
        "        class_names.append(class_name)\n",
        "\n",
        "#         c = time.time()-t\n",
        "#         print('c:',c)\n",
        "        \n",
        "    data = None\n",
        "    labels = None\n",
        "    \n",
        "    #randomize the dataset \n",
        "    permutation = np.random.permutation(y.shape[0])\n",
        "    x = x[permutation, :]\n",
        "    y = y[permutation]\n",
        "\n",
        "    #separate into training and testing \n",
        "    vfold_size = int(x.shape[0]/100*(vfold_ratio*100))\n",
        "\n",
        "    x_test = x[0:vfold_size, :]\n",
        "    y_test = y[0:vfold_size]\n",
        "\n",
        "    x_train = x[vfold_size:x.shape[0], :]\n",
        "    y_train = y[vfold_size:y.shape[0]]\n",
        "    return x_train, y_train, x_test, y_test, class_names"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pZvLn8NuJxCZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# class数量\n",
        "num_classes=345\n",
        "image_size = 28"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rL6XAb4hBMSc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 准备model"
      ]
    },
    {
      "metadata": {
        "id": "uYUVV2wf2z8H",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K6uUjN-WL2Y9",
        "colab_type": "code",
        "outputId": "f5cb9d4d-f488-4cfa-d37e-8e534065bbba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5773
        }
      },
      "cell_type": "code",
      "source": [
        "#x_train, y_train, x_test, y_test, class_names = load_data('data')\n",
        "#num_classes = len(class_names)\n",
        "#print(len(x_train))\n",
        "data_process_count=20\n",
        "count=1\n",
        "while(count<=data_process_count):\n",
        "  # load\n",
        "  x_train, y_train, x_test, y_test, class_names = load_data_process('data_process2',count)\n",
        "  num_classes = len(class_names)\n",
        "\n",
        "  # show random data\n",
        "  import matplotlib.pyplot as plt\n",
        "  from random import randint\n",
        "  %matplotlib inline  \n",
        "  idx = randint(0, len(x_train))\n",
        "  plt.imshow(x_train[idx].reshape(28,28)) \n",
        "  print(class_names[int(y_train[idx].item())])\n",
        "    \n",
        "  # Reshape and normalize\n",
        "  x_train = x_train.reshape(x_train.shape[0], image_size, image_size, 1).astype('float32')/255.0\n",
        "  x_test = x_test.reshape(x_test.shape[0], image_size, image_size, 1).astype('float32')/255.0\n",
        "\n",
        "  # Convert class vectors to class matrices\n",
        "  y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "  y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "  \n",
        "  if count==1:\n",
        "  # Define model\n",
        "    model = keras.Sequential()\n",
        "    model.add(layers.Convolution2D(16, (3, 3),\n",
        "                            padding='same',\n",
        "                            input_shape=x_train.shape[1:], activation='relu'))\n",
        "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(layers.Convolution2D(32, (3, 3), padding='same', activation= 'relu'))\n",
        "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(layers.Convolution2D(64, (3, 3), padding='same', activation= 'relu'))\n",
        "    model.add(layers.MaxPooling2D(pool_size =(2,2)))\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(128, activation='relu'))\n",
        "    model.add(layers.Dense(num_classes, activation='softmax')) \n",
        "    # Train model\n",
        "    adam = tf.train.AdamOptimizer()\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer=adam,\n",
        "                  metrics=['top_k_categorical_accuracy'])\n",
        "    print(model.summary())\n",
        "  \n",
        "  model.fit(x = x_train, y = y_train, validation_split=0.1, batch_size = 256, verbose=2, epochs=5)\n",
        "  \n",
        "  score = model.evaluate(x_test, y_test, verbose=0)\n",
        "  print('Test accuarcy: {:0.2f}%'.format(score[1] * 100))\n",
        "  count+=1\n"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "allfile: 345\n",
            "underwear_1\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_27 (Conv2D)           (None, 28, 28, 16)        160       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_27 (MaxPooling (None, 14, 14, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_28 (Conv2D)           (None, 14, 14, 32)        4640      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_28 (MaxPooling (None, 7, 7, 32)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_29 (Conv2D)           (None, 7, 7, 64)          18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_29 (MaxPooling (None, 3, 3, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_9 (Flatten)          (None, 576)               0         \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 128)               73856     \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 345)               44505     \n",
            "=================================================================\n",
            "Total params: 141,657\n",
            "Trainable params: 141,657\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 248400 samples, validate on 27600 samples\n",
            "Epoch 1/5\n",
            " - 12s - loss: 3.6791 - top_k_categorical_accuracy: 0.4730 - val_loss: 2.9019 - val_top_k_categorical_accuracy: 0.6390\n",
            "Epoch 2/5\n",
            " - 12s - loss: 2.6518 - top_k_categorical_accuracy: 0.6834 - val_loss: 2.5255 - val_top_k_categorical_accuracy: 0.7083\n",
            "Epoch 3/5\n",
            " - 12s - loss: 2.3695 - top_k_categorical_accuracy: 0.7326 - val_loss: 2.3617 - val_top_k_categorical_accuracy: 0.7380\n",
            "Epoch 4/5\n",
            " - 12s - loss: 2.2091 - top_k_categorical_accuracy: 0.7584 - val_loss: 2.2671 - val_top_k_categorical_accuracy: 0.7524\n",
            "Epoch 5/5\n",
            " - 12s - loss: 2.1001 - top_k_categorical_accuracy: 0.7758 - val_loss: 2.1661 - val_top_k_categorical_accuracy: 0.7682\n",
            "Test accuarcy: 77.35%\n",
            "allfile: 345\n",
            "picture_frame_2\n",
            "Train on 248400 samples, validate on 27600 samples\n",
            "Epoch 1/5\n",
            " - 12s - loss: 5.0904 - top_k_categorical_accuracy: 0.1233 - val_loss: 4.4101 - val_top_k_categorical_accuracy: 0.2848\n",
            "Epoch 2/5\n",
            " - 12s - loss: 3.6582 - top_k_categorical_accuracy: 0.4768 - val_loss: 3.1616 - val_top_k_categorical_accuracy: 0.5832\n",
            "Epoch 3/5\n",
            " - 12s - loss: 2.7806 - top_k_categorical_accuracy: 0.6619 - val_loss: 2.5776 - val_top_k_categorical_accuracy: 0.6973\n",
            "Epoch 4/5\n",
            " - 12s - loss: 2.4082 - top_k_categorical_accuracy: 0.7280 - val_loss: 2.3360 - val_top_k_categorical_accuracy: 0.7408\n",
            "Epoch 5/5\n",
            " - 12s - loss: 2.2314 - top_k_categorical_accuracy: 0.7578 - val_loss: 2.2284 - val_top_k_categorical_accuracy: 0.7589\n",
            "Test accuarcy: 75.78%\n",
            "allfile: 345\n",
            "cup_3\n",
            "Train on 248400 samples, validate on 27600 samples\n",
            "Epoch 1/5\n",
            " - 12s - loss: 3.6356 - top_k_categorical_accuracy: 0.4896 - val_loss: 2.8196 - val_top_k_categorical_accuracy: 0.6555\n",
            "Epoch 2/5\n",
            " - 12s - loss: 2.6023 - top_k_categorical_accuracy: 0.6964 - val_loss: 2.4852 - val_top_k_categorical_accuracy: 0.7180\n",
            "Epoch 3/5\n",
            " - 12s - loss: 2.3699 - top_k_categorical_accuracy: 0.7359 - val_loss: 2.3386 - val_top_k_categorical_accuracy: 0.7396\n",
            "Epoch 4/5\n",
            " - 12s - loss: 2.2322 - top_k_categorical_accuracy: 0.7579 - val_loss: 2.2301 - val_top_k_categorical_accuracy: 0.7583\n",
            "Epoch 5/5\n",
            " - 12s - loss: 2.1299 - top_k_categorical_accuracy: 0.7733 - val_loss: 2.1561 - val_top_k_categorical_accuracy: 0.7692\n",
            "Test accuarcy: 76.97%\n",
            "allfile: 345\n",
            "dog_4\n",
            "Train on 248400 samples, validate on 27600 samples\n",
            "Epoch 1/5\n",
            " - 12s - loss: 4.2290 - top_k_categorical_accuracy: 0.3580 - val_loss: 3.1535 - val_top_k_categorical_accuracy: 0.5926\n",
            "Epoch 2/5\n",
            " - 12s - loss: 2.7798 - top_k_categorical_accuracy: 0.6635 - val_loss: 2.4966 - val_top_k_categorical_accuracy: 0.7145\n",
            "Epoch 3/5\n",
            " - 12s - loss: 2.3502 - top_k_categorical_accuracy: 0.7390 - val_loss: 2.2655 - val_top_k_categorical_accuracy: 0.7552\n",
            "Epoch 4/5\n",
            " - 12s - loss: 2.1712 - top_k_categorical_accuracy: 0.7670 - val_loss: 2.1571 - val_top_k_categorical_accuracy: 0.7717\n",
            "Epoch 5/5\n",
            " - 12s - loss: 2.0728 - top_k_categorical_accuracy: 0.7816 - val_loss: 2.0961 - val_top_k_categorical_accuracy: 0.7817\n",
            "Test accuarcy: 77.76%\n",
            "allfile: 345\n",
            "aircraft_carrier_5\n",
            "Train on 248400 samples, validate on 27600 samples\n",
            "Epoch 1/5\n",
            " - 12s - loss: 3.9156 - top_k_categorical_accuracy: 0.4266 - val_loss: 2.8171 - val_top_k_categorical_accuracy: 0.6596\n",
            "Epoch 2/5\n",
            " - 12s - loss: 2.5403 - top_k_categorical_accuracy: 0.7071 - val_loss: 2.3812 - val_top_k_categorical_accuracy: 0.7368\n",
            "Epoch 3/5\n",
            " - 12s - loss: 2.2555 - top_k_categorical_accuracy: 0.7543 - val_loss: 2.2131 - val_top_k_categorical_accuracy: 0.7608\n",
            "Epoch 4/5\n",
            " - 12s - loss: 2.1112 - top_k_categorical_accuracy: 0.7764 - val_loss: 2.1308 - val_top_k_categorical_accuracy: 0.7742\n",
            "Epoch 5/5\n",
            " - 12s - loss: 2.0200 - top_k_categorical_accuracy: 0.7906 - val_loss: 2.0606 - val_top_k_categorical_accuracy: 0.7855\n",
            "Test accuarcy: 78.15%\n",
            "allfile: 345\n",
            "carrot_6\n",
            "Train on 248400 samples, validate on 27600 samples\n",
            "Epoch 1/5\n",
            " - 12s - loss: 4.7743 - top_k_categorical_accuracy: 0.2404 - val_loss: 3.5369 - val_top_k_categorical_accuracy: 0.5110\n",
            "Epoch 2/5\n",
            " - 12s - loss: 3.0515 - top_k_categorical_accuracy: 0.6114 - val_loss: 2.6936 - val_top_k_categorical_accuracy: 0.6811\n",
            "Epoch 3/5\n",
            " - 12s - loss: 2.5115 - top_k_categorical_accuracy: 0.7125 - val_loss: 2.3904 - val_top_k_categorical_accuracy: 0.7336\n",
            "Epoch 4/5\n",
            " - 12s - loss: 2.2721 - top_k_categorical_accuracy: 0.7531 - val_loss: 2.2424 - val_top_k_categorical_accuracy: 0.7571\n",
            "Epoch 5/5\n",
            " - 12s - loss: 2.1408 - top_k_categorical_accuracy: 0.7729 - val_loss: 2.1575 - val_top_k_categorical_accuracy: 0.7696\n",
            "Test accuarcy: 77.08%\n",
            "allfile: 345\n",
            "fork_7\n",
            "Train on 248400 samples, validate on 27600 samples\n",
            "Epoch 1/5\n",
            " - 12s - loss: 4.1289 - top_k_categorical_accuracy: 0.3852 - val_loss: 2.8705 - val_top_k_categorical_accuracy: 0.6481\n",
            "Epoch 2/5\n",
            " - 12s - loss: 2.5421 - top_k_categorical_accuracy: 0.7066 - val_loss: 2.3570 - val_top_k_categorical_accuracy: 0.7435\n",
            "Epoch 3/5\n",
            " - 12s - loss: 2.2290 - top_k_categorical_accuracy: 0.7585 - val_loss: 2.2175 - val_top_k_categorical_accuracy: 0.7640\n",
            "Epoch 4/5\n",
            " - 12s - loss: 2.0945 - top_k_categorical_accuracy: 0.7787 - val_loss: 2.1330 - val_top_k_categorical_accuracy: 0.7768\n",
            "Epoch 5/5\n",
            " - 12s - loss: 2.0061 - top_k_categorical_accuracy: 0.7921 - val_loss: 2.0630 - val_top_k_categorical_accuracy: 0.7873\n",
            "Test accuarcy: 78.56%\n",
            "allfile: 345\n",
            "bear_8\n",
            "Train on 248400 samples, validate on 27600 samples\n",
            "Epoch 1/5\n",
            " - 12s - loss: 4.8157 - top_k_categorical_accuracy: 0.2341 - val_loss: 3.4251 - val_top_k_categorical_accuracy: 0.5245\n",
            "Epoch 2/5\n",
            " - 12s - loss: 2.8731 - top_k_categorical_accuracy: 0.6444 - val_loss: 2.5711 - val_top_k_categorical_accuracy: 0.7047\n",
            "Epoch 3/5\n",
            " - 12s - loss: 2.3988 - top_k_categorical_accuracy: 0.7298 - val_loss: 2.3485 - val_top_k_categorical_accuracy: 0.7385\n",
            "Epoch 4/5\n",
            " - 12s - loss: 2.2132 - top_k_categorical_accuracy: 0.7602 - val_loss: 2.2585 - val_top_k_categorical_accuracy: 0.7572\n",
            "Epoch 5/5\n",
            " - 12s - loss: 2.0979 - top_k_categorical_accuracy: 0.7783 - val_loss: 2.1731 - val_top_k_categorical_accuracy: 0.7684\n",
            "Test accuarcy: 77.00%\n",
            "allfile: 345\n",
            "camera_9\n",
            "Train on 248400 samples, validate on 27600 samples\n",
            "Epoch 1/5\n",
            " - 12s - loss: 5.1597 - top_k_categorical_accuracy: 0.1531 - val_loss: 4.1668 - val_top_k_categorical_accuracy: 0.3628\n",
            "Epoch 2/5\n",
            " - 12s - loss: 3.6128 - top_k_categorical_accuracy: 0.4907 - val_loss: 3.2089 - val_top_k_categorical_accuracy: 0.5782\n",
            "Epoch 3/5\n",
            " - 12s - loss: 2.8553 - top_k_categorical_accuracy: 0.6486 - val_loss: 2.6562 - val_top_k_categorical_accuracy: 0.6883\n",
            "Epoch 4/5\n",
            " - 12s - loss: 2.4466 - top_k_categorical_accuracy: 0.7231 - val_loss: 2.3614 - val_top_k_categorical_accuracy: 0.7384\n",
            "Epoch 5/5\n",
            " - 12s - loss: 2.2231 - top_k_categorical_accuracy: 0.7594 - val_loss: 2.2267 - val_top_k_categorical_accuracy: 0.7617\n",
            "Test accuarcy: 75.98%\n",
            "allfile: 345\n",
            "hospital_10\n",
            "Train on 248400 samples, validate on 27600 samples\n",
            "Epoch 1/5\n",
            " - 12s - loss: 4.8331 - top_k_categorical_accuracy: 0.2213 - val_loss: 3.7670 - val_top_k_categorical_accuracy: 0.4538\n",
            "Epoch 2/5\n",
            " - 12s - loss: 3.3631 - top_k_categorical_accuracy: 0.5459 - val_loss: 3.0405 - val_top_k_categorical_accuracy: 0.6138\n",
            "Epoch 3/5\n",
            " - 12s - loss: 2.8118 - top_k_categorical_accuracy: 0.6583 - val_loss: 2.6581 - val_top_k_categorical_accuracy: 0.6878\n",
            "Epoch 4/5\n",
            " - 12s - loss: 2.4753 - top_k_categorical_accuracy: 0.7174 - val_loss: 2.4049 - val_top_k_categorical_accuracy: 0.7317\n",
            "Epoch 5/5\n",
            " - 12s - loss: 2.2708 - top_k_categorical_accuracy: 0.7511 - val_loss: 2.2616 - val_top_k_categorical_accuracy: 0.7565\n",
            "Test accuarcy: 75.78%\n",
            "allfile: 345\n",
            "foot_11\n",
            "Train on 248400 samples, validate on 27600 samples\n",
            "Epoch 1/5\n",
            " - 12s - loss: 4.5348 - top_k_categorical_accuracy: 0.2960 - val_loss: 3.4837 - val_top_k_categorical_accuracy: 0.5189\n",
            "Epoch 2/5\n",
            " - 12s - loss: 3.0513 - top_k_categorical_accuracy: 0.6107 - val_loss: 2.8082 - val_top_k_categorical_accuracy: 0.6577\n",
            "Epoch 3/5\n",
            " - 12s - loss: 2.5897 - top_k_categorical_accuracy: 0.6967 - val_loss: 2.5005 - val_top_k_categorical_accuracy: 0.7149\n",
            "Epoch 4/5\n",
            " - 12s - loss: 2.3579 - top_k_categorical_accuracy: 0.7361 - val_loss: 2.3526 - val_top_k_categorical_accuracy: 0.7375\n",
            "Epoch 5/5\n",
            " - 12s - loss: 2.2177 - top_k_categorical_accuracy: 0.7586 - val_loss: 2.2595 - val_top_k_categorical_accuracy: 0.7541\n",
            "Test accuarcy: 75.60%\n",
            "allfile: 345\n",
            "bracelet_12\n",
            "Train on 248400 samples, validate on 27600 samples\n",
            "Epoch 1/5\n",
            " - 12s - loss: 4.6779 - top_k_categorical_accuracy: 0.2559 - val_loss: 3.5773 - val_top_k_categorical_accuracy: 0.4996\n",
            "Epoch 2/5\n",
            " - 12s - loss: 3.2012 - top_k_categorical_accuracy: 0.5783 - val_loss: 2.9266 - val_top_k_categorical_accuracy: 0.6311\n",
            "Epoch 3/5\n",
            " - 12s - loss: 2.7238 - top_k_categorical_accuracy: 0.6710 - val_loss: 2.5935 - val_top_k_categorical_accuracy: 0.6956\n",
            "Epoch 4/5\n",
            " - 12s - loss: 2.4363 - top_k_categorical_accuracy: 0.7225 - val_loss: 2.3874 - val_top_k_categorical_accuracy: 0.7317\n",
            "Epoch 5/5\n",
            " - 12s - loss: 2.2659 - top_k_categorical_accuracy: 0.7513 - val_loss: 2.2687 - val_top_k_categorical_accuracy: 0.7499\n",
            "Test accuarcy: 75.25%\n",
            "allfile: 345\n",
            "pineapple_13\n",
            "Train on 248400 samples, validate on 27600 samples\n",
            "Epoch 1/5\n",
            " - 12s - loss: 4.5541 - top_k_categorical_accuracy: 0.2808 - val_loss: 3.5394 - val_top_k_categorical_accuracy: 0.5052\n",
            "Epoch 2/5\n",
            " - 12s - loss: 3.1032 - top_k_categorical_accuracy: 0.5984 - val_loss: 2.8044 - val_top_k_categorical_accuracy: 0.6592\n",
            "Epoch 3/5\n",
            " - 12s - loss: 2.6107 - top_k_categorical_accuracy: 0.6940 - val_loss: 2.5072 - val_top_k_categorical_accuracy: 0.7127\n",
            "Epoch 4/5\n",
            " - 12s - loss: 2.3733 - top_k_categorical_accuracy: 0.7353 - val_loss: 2.3398 - val_top_k_categorical_accuracy: 0.7405\n",
            "Epoch 5/5\n",
            " - 12s - loss: 2.2276 - top_k_categorical_accuracy: 0.7590 - val_loss: 2.2439 - val_top_k_categorical_accuracy: 0.7564\n",
            "Test accuarcy: 75.87%\n",
            "allfile: 345\n",
            "foot_14\n",
            "Train on 248400 samples, validate on 27600 samples\n",
            "Epoch 1/5\n",
            " - 12s - loss: 4.1963 - top_k_categorical_accuracy: 0.3691 - val_loss: 3.1339 - val_top_k_categorical_accuracy: 0.5966\n",
            "Epoch 2/5\n",
            " - 12s - loss: 2.7817 - top_k_categorical_accuracy: 0.6630 - val_loss: 2.5511 - val_top_k_categorical_accuracy: 0.7050\n",
            "Epoch 3/5\n",
            " - 12s - loss: 2.4081 - top_k_categorical_accuracy: 0.7294 - val_loss: 2.3524 - val_top_k_categorical_accuracy: 0.7393\n",
            "Epoch 4/5\n",
            " - 12s - loss: 2.2355 - top_k_categorical_accuracy: 0.7573 - val_loss: 2.2361 - val_top_k_categorical_accuracy: 0.7582\n",
            "Epoch 5/5\n",
            " - 12s - loss: 2.1312 - top_k_categorical_accuracy: 0.7734 - val_loss: 2.1779 - val_top_k_categorical_accuracy: 0.7671\n",
            "Test accuarcy: 76.78%\n",
            "allfile: 345\n",
            "swing_set_15\n",
            "Train on 248400 samples, validate on 27600 samples\n",
            "Epoch 1/5\n",
            " - 12s - loss: 4.9420 - top_k_categorical_accuracy: 0.2017 - val_loss: 3.8286 - val_top_k_categorical_accuracy: 0.4424\n",
            "Epoch 2/5\n",
            " - 12s - loss: 3.3850 - top_k_categorical_accuracy: 0.5409 - val_loss: 3.0531 - val_top_k_categorical_accuracy: 0.6128\n",
            "Epoch 3/5\n",
            " - 12s - loss: 2.8310 - top_k_categorical_accuracy: 0.6530 - val_loss: 2.6655 - val_top_k_categorical_accuracy: 0.6837\n",
            "Epoch 4/5\n",
            " - 12s - loss: 2.5248 - top_k_categorical_accuracy: 0.7081 - val_loss: 2.4399 - val_top_k_categorical_accuracy: 0.7224\n",
            "Epoch 5/5\n",
            " - 12s - loss: 2.3330 - top_k_categorical_accuracy: 0.7412 - val_loss: 2.3158 - val_top_k_categorical_accuracy: 0.7437\n",
            "Test accuarcy: 74.33%\n",
            "allfile: 345\n",
            "cookie_16\n",
            "Train on 248400 samples, validate on 27600 samples\n",
            "Epoch 1/5\n",
            " - 12s - loss: 4.4532 - top_k_categorical_accuracy: 0.3079 - val_loss: 3.4334 - val_top_k_categorical_accuracy: 0.5323\n",
            "Epoch 2/5\n",
            " - 12s - loss: 3.0721 - top_k_categorical_accuracy: 0.6059 - val_loss: 2.8222 - val_top_k_categorical_accuracy: 0.6545\n",
            "Epoch 3/5\n",
            " - 12s - loss: 2.6164 - top_k_categorical_accuracy: 0.6915 - val_loss: 2.5023 - val_top_k_categorical_accuracy: 0.7137\n",
            "Epoch 4/5\n",
            " - 12s - loss: 2.3538 - top_k_categorical_accuracy: 0.7379 - val_loss: 2.3210 - val_top_k_categorical_accuracy: 0.7446\n",
            "Epoch 5/5\n",
            " - 12s - loss: 2.2083 - top_k_categorical_accuracy: 0.7609 - val_loss: 2.2417 - val_top_k_categorical_accuracy: 0.7571\n",
            "Test accuarcy: 76.03%\n",
            "allfile: 345\n",
            "cruise_ship_17\n",
            "Train on 248400 samples, validate on 27600 samples\n",
            "Epoch 1/5\n",
            " - 12s - loss: 4.6535 - top_k_categorical_accuracy: 0.2685 - val_loss: 3.5107 - val_top_k_categorical_accuracy: 0.5210\n",
            "Epoch 2/5\n",
            " - 12s - loss: 3.0236 - top_k_categorical_accuracy: 0.6168 - val_loss: 2.7330 - val_top_k_categorical_accuracy: 0.6751\n",
            "Epoch 3/5\n",
            " - 12s - loss: 2.5354 - top_k_categorical_accuracy: 0.7062 - val_loss: 2.4542 - val_top_k_categorical_accuracy: 0.7220\n",
            "Epoch 4/5\n",
            " - 12s - loss: 2.3228 - top_k_categorical_accuracy: 0.7426 - val_loss: 2.3029 - val_top_k_categorical_accuracy: 0.7495\n",
            "Epoch 5/5\n",
            " - 12s - loss: 2.1920 - top_k_categorical_accuracy: 0.7636 - val_loss: 2.2211 - val_top_k_categorical_accuracy: 0.7606\n",
            "Test accuarcy: 76.06%\n",
            "allfile: 345\n",
            "waterslide_18\n",
            "Train on 248400 samples, validate on 27600 samples\n",
            "Epoch 1/5\n",
            " - 12s - loss: 5.0830 - top_k_categorical_accuracy: 0.1753 - val_loss: 4.0619 - val_top_k_categorical_accuracy: 0.3910\n",
            "Epoch 2/5\n",
            " - 12s - loss: 3.6642 - top_k_categorical_accuracy: 0.4805 - val_loss: 3.4319 - val_top_k_categorical_accuracy: 0.5352\n",
            "Epoch 3/5\n",
            " - 12s - loss: 3.1720 - top_k_categorical_accuracy: 0.5859 - val_loss: 3.0313 - val_top_k_categorical_accuracy: 0.6131\n",
            "Epoch 4/5\n",
            " - 12s - loss: 2.8682 - top_k_categorical_accuracy: 0.6461 - val_loss: 2.8116 - val_top_k_categorical_accuracy: 0.6545\n",
            "Epoch 5/5\n",
            " - 12s - loss: 2.6606 - top_k_categorical_accuracy: 0.6850 - val_loss: 2.6350 - val_top_k_categorical_accuracy: 0.6888\n",
            "Test accuarcy: 69.59%\n",
            "allfile: 345\n",
            "purse_19\n",
            "Train on 248400 samples, validate on 27600 samples\n",
            "Epoch 1/5\n",
            " - 12s - loss: 4.6425 - top_k_categorical_accuracy: 0.2742 - val_loss: 3.5103 - val_top_k_categorical_accuracy: 0.5152\n",
            "Epoch 2/5\n",
            " - 12s - loss: 3.0505 - top_k_categorical_accuracy: 0.6068 - val_loss: 2.7864 - val_top_k_categorical_accuracy: 0.6610\n",
            "Epoch 3/5\n",
            " - 12s - loss: 2.5902 - top_k_categorical_accuracy: 0.6956 - val_loss: 2.5202 - val_top_k_categorical_accuracy: 0.7095\n",
            "Epoch 4/5\n",
            " - 12s - loss: 2.3819 - top_k_categorical_accuracy: 0.7314 - val_loss: 2.3931 - val_top_k_categorical_accuracy: 0.7309\n",
            "Epoch 5/5\n",
            " - 12s - loss: 2.2539 - top_k_categorical_accuracy: 0.7520 - val_loss: 2.2999 - val_top_k_categorical_accuracy: 0.7472\n",
            "Test accuarcy: 75.02%\n",
            "allfile: 345\n",
            "pear_20\n",
            "Train on 248400 samples, validate on 27600 samples\n",
            "Epoch 1/5\n",
            " - 12s - loss: 4.8997 - top_k_categorical_accuracy: 0.2181 - val_loss: 3.7014 - val_top_k_categorical_accuracy: 0.4746\n",
            "Epoch 2/5\n",
            " - 12s - loss: 3.2134 - top_k_categorical_accuracy: 0.5785 - val_loss: 2.9303 - val_top_k_categorical_accuracy: 0.6355\n",
            "Epoch 3/5\n",
            " - 12s - loss: 2.7211 - top_k_categorical_accuracy: 0.6736 - val_loss: 2.6210 - val_top_k_categorical_accuracy: 0.6887\n",
            "Epoch 4/5\n",
            " - 12s - loss: 2.4785 - top_k_categorical_accuracy: 0.7157 - val_loss: 2.4733 - val_top_k_categorical_accuracy: 0.7166\n",
            "Epoch 5/5\n",
            " - 12s - loss: 2.3307 - top_k_categorical_accuracy: 0.7404 - val_loss: 2.3687 - val_top_k_categorical_accuracy: 0.7350\n",
            "Test accuarcy: 73.66%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPoAAAD4CAYAAADFJPs2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "_YRSRkOyBP1P",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Training "
      ]
    },
    {
      "metadata": {
        "id": "7OMEJ7kF3lsP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "outputId": "00cf0861-5931-43e5-dd1a-4ad2a4ec2742"
      },
      "cell_type": "code",
      "source": [
        "model.fit(x = x_train, y = y_train, validation_split=0.1, batch_size = 256, verbose=2, epochs=5)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 248400 samples, validate on 27600 samples\n",
            "Epoch 1/5\n",
            " - 12s - loss: 2.2227 - top_k_categorical_accuracy: 0.7584 - val_loss: 2.2714 - val_top_k_categorical_accuracy: 0.7492\n",
            "Epoch 2/5\n",
            " - 12s - loss: 2.1443 - top_k_categorical_accuracy: 0.7704 - val_loss: 2.2254 - val_top_k_categorical_accuracy: 0.7567\n",
            "Epoch 3/5\n",
            " - 12s - loss: 2.0830 - top_k_categorical_accuracy: 0.7795 - val_loss: 2.1819 - val_top_k_categorical_accuracy: 0.7643\n",
            "Epoch 4/5\n",
            " - 12s - loss: 2.0316 - top_k_categorical_accuracy: 0.7877 - val_loss: 2.1486 - val_top_k_categorical_accuracy: 0.7679\n",
            "Epoch 5/5\n",
            " - 12s - loss: 1.9865 - top_k_categorical_accuracy: 0.7942 - val_loss: 2.1185 - val_top_k_categorical_accuracy: 0.7729\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f92eba90ef0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "metadata": {
        "id": "d2KztY7qEn9_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Testing "
      ]
    },
    {
      "metadata": {
        "id": "ssaZczS7DxeA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b5400823-aafb-4e7c-e5b7-47fa1c7b63fa"
      },
      "cell_type": "code",
      "source": [
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test accuarcy: {:0.2f}%'.format(score[1] * 100))"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuarcy: 77.66%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9xBM_w0VBbNr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Inference "
      ]
    },
    {
      "metadata": {
        "id": "nH3JfoiYHdpk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "0fc142ca-e600-42b5-8240-69b69fb6195a"
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from random import randint\n",
        "%matplotlib inline  \n",
        "idx = randint(0, len(x_test))\n",
        "img = x_test[idx]\n",
        "plt.imshow(img.squeeze()) \n",
        "pred = model.predict(np.expand_dims(img, axis=0))[0]\n",
        "ind = (-pred).argsort()[:5]\n",
        "latex = [class_names[x] for x in ind]\n",
        "print(latex)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['snorkel_20', 'beard_20', 'radio_20', 'hockey_puck_20', 'grapes_20']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPoAAAD4CAYAAADFJPs2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngIIu4oCCLuKAgi7igIIu4sD/A7PV0khlTvT6AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "YPp5D82YBhM-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Store the classes "
      ]
    },
    {
      "metadata": {
        "id": "NoFI1msFYpCN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with open('class_names.txt', 'w') as file_handler:\n",
        "    for item in class_names:\n",
        "        file_handler.write(\"{}\\n\".format(item))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mfJ6dpaDBpRx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Install TensorFlowJS"
      ]
    },
    {
      "metadata": {
        "id": "hJJDfp9mY9Xh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 865
        },
        "outputId": "53668926-800c-4782-9766-a5ef7117a2a4"
      },
      "cell_type": "code",
      "source": [
        "!pip install tensorflowjs "
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflowjs\n",
            "  Downloading https://files.pythonhosted.org/packages/79/29/35e1aa467436ff46b98df65a08c49faaedb3429e1c512d1d90fe308040a0/tensorflowjs-1.0.1-py3-none-any.whl\n",
            "Requirement already satisfied: six==1.11.0 in /usr/local/lib/python3.6/dist-packages (from tensorflowjs) (1.11.0)\n",
            "Requirement already satisfied: tensorflow-hub==0.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflowjs) (0.3.0)\n",
            "Collecting tf-nightly-2.0-preview>=2.0.0.dev20190304 (from tensorflowjs)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/51/9d/5b4d7fbf2e4a3730dde312e0362991e6d1cdc9312244b3ebe88268e2d08c/tf_nightly_2.0_preview-2.0.0.dev20190320-cp36-cp36m-manylinux1_x86_64.whl (80.2MB)\n",
            "\u001b[K    100% |████████████████████████████████| 80.2MB 339kB/s \n",
            "\u001b[?25hCollecting numpy==1.15.1 (from tensorflowjs)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fe/94/7049fed8373c52839c8cde619acaf2c9b83082b935e5aa8c0fa27a4a8bcc/numpy-1.15.1-cp36-cp36m-manylinux1_x86_64.whl (13.9MB)\n",
            "\u001b[K    100% |████████████████████████████████| 13.9MB 293kB/s \n",
            "\u001b[?25hRequirement already satisfied: keras==2.2.4 in /usr/local/lib/python3.6/dist-packages (from tensorflowjs) (2.2.4)\n",
            "Requirement already satisfied: h5py==2.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflowjs) (2.8.0)\n",
            "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub==0.3.0->tensorflowjs) (3.7.0)\n",
            "Collecting tb-nightly<1.15.0a0,>=1.14.0a0 (from tf-nightly-2.0-preview>=2.0.0.dev20190304->tensorflowjs)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5d/17/a3d05a0664c11703259aa79d2b58b871b3bb1fff24153f75db04540489db/tb_nightly-1.14.0a20190319-py3-none-any.whl (3.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 3.0MB 6.5MB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator-2.0-preview (from tf-nightly-2.0-preview>=2.0.0.dev20190304->tensorflowjs)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/01/cd/eb5b6c9bfad9b39b771726d2759bd9792056a39dba6bbfce3d86336376b7/tensorflow_estimator_2.0_preview-1.14.0.dev2019032000-py2.py3-none-any.whl (351kB)\n",
            "\u001b[K    100% |████████████████████████████████| 358kB 9.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview>=2.0.0.dev20190304->tensorflowjs) (1.15.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview>=2.0.0.dev20190304->tensorflowjs) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview>=2.0.0.dev20190304->tensorflowjs) (0.33.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview>=2.0.0.dev20190304->tensorflowjs) (1.0.7)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview>=2.0.0.dev20190304->tensorflowjs) (0.7.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview>=2.0.0.dev20190304->tensorflowjs) (0.2.2)\n",
            "Collecting google-pasta>=0.1.2 (from tf-nightly-2.0-preview>=2.0.0.dev20190304->tensorflowjs)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8c/96/adbd4eafe72ce9b5ca6f168fbf109386e1b601f7c59926a11e9d7b7a5b44/google_pasta-0.1.4-py3-none-any.whl (51kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 21.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview>=2.0.0.dev20190304->tensorflowjs) (0.7.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview>=2.0.0.dev20190304->tensorflowjs) (1.0.9)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4->tensorflowjs) (1.1.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4->tensorflowjs) (3.13)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.4.0->tensorflow-hub==0.3.0->tensorflowjs) (40.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.15.0a0,>=1.14.0a0->tf-nightly-2.0-preview>=2.0.0.dev20190304->tensorflowjs) (3.0.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.15.0a0,>=1.14.0a0->tf-nightly-2.0-preview>=2.0.0.dev20190304->tensorflowjs) (0.14.1)\n",
            "\u001b[31mfeaturetools 0.4.1 has requirement pandas>=0.23.0, but you'll have pandas 0.22.0 which is incompatible.\u001b[0m\n",
            "\u001b[31malbumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.8 which is incompatible.\u001b[0m\n",
            "Installing collected packages: numpy, tb-nightly, tensorflow-estimator-2.0-preview, google-pasta, tf-nightly-2.0-preview, tensorflowjs\n",
            "  Found existing installation: numpy 1.14.6\n",
            "    Uninstalling numpy-1.14.6:\n",
            "      Successfully uninstalled numpy-1.14.6\n",
            "Successfully installed google-pasta-0.1.4 numpy-1.15.1 tb-nightly-1.14.0a20190319 tensorflow-estimator-2.0-preview-1.14.0.dev2019032000 tensorflowjs-1.0.1 tf-nightly-2.0-preview-2.0.0.dev20190320\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "tensorflow",
                  "tensorflow_estimator"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "-oBl0ZKVB00d",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Save and Convert "
      ]
    },
    {
      "metadata": {
        "id": "XVICB3TbZGb2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "a8ad0bf2-8e8b-4447-8893-63e4ad731301"
      },
      "cell_type": "code",
      "source": [
        "model.save('keras.h5')"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bTWWlGdWZOvs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mkdir model\n",
        "!tensorflowjs_converter --input_format keras keras.h5 model/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JKYxE2MEB6LV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Zip and Download "
      ]
    },
    {
      "metadata": {
        "id": "865-t79uaB63",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!cp class_names.txt model/class_names.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GLC-MzW8ZXTa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "dad49da0-aafc-4e8f-f620-e00257f15f67"
      },
      "cell_type": "code",
      "source": [
        "!zip -r model.zip model "
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: model/ (stored 0%)\n",
            "  adding: model/group1-shard1of1.bin (deflated 7%)\n",
            "  adding: model/class_names.txt (deflated 57%)\n",
            "  adding: model/model.json (deflated 85%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4vfPR03xZZeD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('model.zip')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}