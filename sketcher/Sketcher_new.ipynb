{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sketcher.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hlxabcd/hlxabcd.github.io/blob/master/sketcher/Sketcher_new.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "zlx6-LFL_jbi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This file contains a subset of the quick draw classes. I choose around 100 classes from the dataset. "
      ]
    },
    {
      "metadata": {
        "id": "6H3ATAdp_URp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Get the Class names "
      ]
    },
    {
      "metadata": {
        "id": "XXv-xzU1sd88",
        "colab_type": "code",
        "outputId": "3e9af9dd-1d5b-4eed-cd80-d12a3a10d754",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        }
      },
      "cell_type": "code",
      "source": [
        "!wget 'https://raw.githubusercontent.com/hlxabcd/hlxabcd.github.io/master/sketcher/class_names.txt'"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-03-19 13:40:00--  https://raw.githubusercontent.com/hlxabcd/hlxabcd.github.io/master/sketcher/class_names.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2790 (2.7K) [text/plain]\n",
            "Saving to: ‘class_names.txt.4’\n",
            "\n",
            "\rclass_names.txt.4     0%[                    ]       0  --.-KB/s               \rclass_names.txt.4   100%[===================>]   2.72K  --.-KB/s    in 0s      \n",
            "\n",
            "2019-03-19 13:40:01 (56.6 MB/s) - ‘class_names.txt.4’ saved [2790/2790]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4GL_TdMffD6-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Read the classes names "
      ]
    },
    {
      "metadata": {
        "id": "eP-OxOx5sy0b",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "f = open(\"class_names.txt\",\"r\")\n",
        "# And for reading use\n",
        "classes = f.readlines()\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lTE6D3uxtMc5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "classes = [c.replace('\\n','').replace(' ','_') for c in classes]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5NDfBHVjACAt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Download the Dataset "
      ]
    },
    {
      "metadata": {
        "id": "7MC_PUS-fKjH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Loop over the classes and download the currospondent data"
      ]
    },
    {
      "metadata": {
        "id": "rdSUnpL0u22Q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mkdir data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "22DPhL5FtWcQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "import os\n",
        "def download():\n",
        "  \n",
        "  base = 'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/'\n",
        "  for c in classes:\n",
        "    cls_url = c.replace('_', '%20')\n",
        "    if not os.path.exists('data/' + c + '.npy'):\n",
        "        path = base+cls_url+'.npy'\n",
        "        print(path)\n",
        "        urllib.request.urlretrieve(path, 'data/'+c+'.npy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O5jF6TXXu-Bu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "download() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uEdnbBVXAI-X",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Imports "
      ]
    },
    {
      "metadata": {
        "id": "J2FYrPgOKh6t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow import keras \n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rxgNFjNM5sWj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 分割npy库提高加载效率"
      ]
    },
    {
      "metadata": {
        "id": "SisxobWa54B-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def split_dataset(root,target,max_items_per_class= 1000 ):\n",
        "     all_files = glob.glob(os.path.join(root, '*.npy'))\n",
        "     for idx, file in enumerate(all_files):\n",
        "        print(\"process npy:\",idx,file)\n",
        "        data = np.load(file)\n",
        "        # npy长度\n",
        "        dataset_len = len(data)\n",
        "        start = 0\n",
        "        end = max_items_per_class\n",
        "        num = 1\n",
        "        fileName = file.split('/')[1].split('.')[0]\n",
        "        # 按max_items_per_class分割npy文件\n",
        "        while (start < dataset_len):\n",
        "            resultData = data[start: end, :]\n",
        "            # 分割后的文件命名\n",
        "            targetFile = os.path.join(target, fileName+'_'+str(num)+'.npy');\n",
        "            if not os.path.exists(targetFile):\n",
        "              np.save(targetFile, resultData)\n",
        "            num+=1\n",
        "            start+=max_items_per_class\n",
        "            end+=max_items_per_class\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CbXnUx_z90Iw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 分割npy\n",
        "#!mkdir data_process2\n",
        "#split_dataset('data','data_process2')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6o30ipBPAQ5Y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Load the Data "
      ]
    },
    {
      "metadata": {
        "id": "UBq3GXEKAYuO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Each class contains different number samples of arrays stored as .npy format. Since we have some memory limitations we only load 5000 images per class.  "
      ]
    },
    {
      "metadata": {
        "id": "6HEIgQNHYQnl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_data(root, vfold_ratio=0.2, max_items_per_class= 2000 ):\n",
        "    all_files = glob.glob(os.path.join(root, '*.npy'))\n",
        "\n",
        "    #initialize variables \n",
        "    x = np.empty([0, 784])\n",
        "    y = np.empty([0])\n",
        "    class_names = []\n",
        "\n",
        "    #load each data file \n",
        "    for idx, file in enumerate(all_files):\n",
        "        print(idx,file)\n",
        "        data = np.load(file)\n",
        "        data = data[0: max_items_per_class, :]\n",
        "        labels = np.full(data.shape[0], idx)\n",
        "\n",
        "        x = np.concatenate((x, data), axis=0)\n",
        "        y = np.append(y, labels)\n",
        "\n",
        "        class_name, ext = os.path.splitext(os.path.basename(file))\n",
        "        class_names.append(class_name)\n",
        "\n",
        "    data = None\n",
        "    labels = None\n",
        "    \n",
        "    #randomize the dataset \n",
        "    permutation = np.random.permutation(y.shape[0])\n",
        "    x = x[permutation, :]\n",
        "    y = y[permutation]\n",
        "\n",
        "    #separate into training and testing \n",
        "    vfold_size = int(x.shape[0]/100*(vfold_ratio*100))\n",
        "\n",
        "    x_test = x[0:vfold_size, :]\n",
        "    y_test = y[0:vfold_size]\n",
        "\n",
        "    x_train = x[vfold_size:x.shape[0], :]\n",
        "    y_train = y[vfold_size:y.shape[0]]\n",
        "    return x_train, y_train, x_test, y_test, class_names\n",
        "\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "\n",
        "  \n",
        "def load_data_process(root, data_process_num, vfold_ratio=0.2):\n",
        "    all_files = glob.glob(os.path.join(root, '*_'+str(data_process_num)+'.npy'))\n",
        "\n",
        "    #initialize variables \n",
        "    x = np.empty([0, 784])\n",
        "    y = np.empty([0])\n",
        "    class_names = []\n",
        "    print(\"allfile:\",len(all_files))\n",
        "    #load each data file \n",
        "    for idx, file in enumerate(all_files):\n",
        "        \n",
        "        t = time.time()\n",
        "        data = np.load(file)\n",
        "        a = time.time()-t\n",
        "        print('a:',a)\n",
        "        labels = np.full(data.shape[0], idx)\n",
        "        x = np.concatenate((x, data), axis=0)\n",
        "        y = np.append(y, labels)\n",
        "        b = time.time()-t\n",
        "        print('b:',b)\n",
        "        class_name, ext = os.path.splitext(os.path.basename(file))\n",
        "        class_names.append(class_name)\n",
        "\n",
        "        c = time.time()-t\n",
        "        print('c:',c)\n",
        "        \n",
        "    data = None\n",
        "    labels = None\n",
        "    \n",
        "    #randomize the dataset \n",
        "    permutation = np.random.permutation(y.shape[0])\n",
        "    x = x[permutation, :]\n",
        "    y = y[permutation]\n",
        "\n",
        "    #separate into training and testing \n",
        "    vfold_size = int(x.shape[0]/100*(vfold_ratio*100))\n",
        "\n",
        "    x_test = x[0:vfold_size, :]\n",
        "    y_test = y[0:vfold_size]\n",
        "\n",
        "    x_train = x[vfold_size:x.shape[0], :]\n",
        "    y_train = y[vfold_size:y.shape[0]]\n",
        "    return x_train, y_train, x_test, y_test, class_names"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pZvLn8NuJxCZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# class数量\n",
        "num_classes=345\n",
        "image_size = 28"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rL6XAb4hBMSc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 准备model"
      ]
    },
    {
      "metadata": {
        "id": "uYUVV2wf2z8H",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K6uUjN-WL2Y9",
        "colab_type": "code",
        "outputId": "36174114-bf6f-445b-f382-40cb1234c0a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 19159
        }
      },
      "cell_type": "code",
      "source": [
        "#x_train, y_train, x_test, y_test, class_names = load_data('data')\n",
        "#num_classes = len(class_names)\n",
        "#print(len(x_train))\n",
        "data_process_count=2\n",
        "count=1\n",
        "while(count<=data_process_count):\n",
        "  # load\n",
        "  x_train, y_train, x_test, y_test, class_names = load_data_process('data_process2',count)\n",
        "  num_classes = len(class_names)\n",
        "\n",
        "  # show random data\n",
        "  import matplotlib.pyplot as plt\n",
        "  from random import randint\n",
        "  %matplotlib inline  \n",
        "  idx = randint(0, len(x_train))\n",
        "  plt.imshow(x_train[idx].reshape(28,28)) \n",
        "  print(class_names[int(y_train[idx].item())])\n",
        "    \n",
        "  # Reshape and normalize\n",
        "  x_train = x_train.reshape(x_train.shape[0], image_size, image_size, 1).astype('float32')/255.0\n",
        "  x_test = x_test.reshape(x_test.shape[0], image_size, image_size, 1).astype('float32')/255.0\n",
        "\n",
        "  # Convert class vectors to class matrices\n",
        "  y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "  y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "  \n",
        "  if count==1:\n",
        "  # Define model\n",
        "    model = keras.Sequential()\n",
        "    model.add(layers.Convolution2D(16, (3, 3),\n",
        "                            padding='same',\n",
        "                            input_shape=x_train.shape[1:], activation='relu'))\n",
        "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(layers.Convolution2D(32, (3, 3), padding='same', activation= 'relu'))\n",
        "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(layers.Convolution2D(64, (3, 3), padding='same', activation= 'relu'))\n",
        "    model.add(layers.MaxPooling2D(pool_size =(2,2)))\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(128, activation='relu'))\n",
        "    model.add(layers.Dense(num_classes, activation='softmax')) \n",
        "    # Train model\n",
        "    adam = tf.train.AdamOptimizer()\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer=adam,\n",
        "                  metrics=['top_k_categorical_accuracy'])\n",
        "    print(model.summary())\n",
        "  \n",
        "  model.fit(x = x_train, y = y_train, validation_split=0.1, batch_size = 256, verbose=2, epochs=5)\n",
        "  \n",
        "  score = model.evaluate(x_test, y_test, verbose=0)\n",
        "  print('Test accuarcy: {:0.2f}%'.format(score[1] * 100))\n"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "allfile: 345\n",
            "a: 0.001317739486694336\n",
            "b: 0.006994009017944336\n",
            "c: 0.0071277618408203125\n",
            "a: 0.0018863677978515625\n",
            "b: 0.015592336654663086\n",
            "c: 0.015770912170410156\n",
            "a: 0.0021164417266845703\n",
            "b: 0.023138046264648438\n",
            "c: 0.02372121810913086\n",
            "a: 0.002078533172607422\n",
            "b: 0.01623678207397461\n",
            "c: 0.01641845703125\n",
            "a: 0.0017075538635253906\n",
            "b: 0.022182703018188477\n",
            "c: 0.022748470306396484\n",
            "a: 0.0020885467529296875\n",
            "b: 0.04616880416870117\n",
            "c: 0.046777963638305664\n",
            "a: 0.001837015151977539\n",
            "b: 0.05702996253967285\n",
            "c: 0.05765700340270996\n",
            "a: 0.0018291473388671875\n",
            "b: 0.06621670722961426\n",
            "c: 0.06691145896911621\n",
            "a: 0.0021638870239257812\n",
            "b: 0.017876386642456055\n",
            "c: 0.018460750579833984\n",
            "a: 0.0019638538360595703\n",
            "b: 0.01761007308959961\n",
            "c: 0.018201828002929688\n",
            "a: 0.002115011215209961\n",
            "b: 0.018523693084716797\n",
            "c: 0.01913762092590332\n",
            "a: 0.002225160598754883\n",
            "b: 0.0209958553314209\n",
            "c: 0.02120804786682129\n",
            "a: 0.002222299575805664\n",
            "b: 0.0329282283782959\n",
            "c: 0.03360152244567871\n",
            "a: 0.002126455307006836\n",
            "b: 0.024205684661865234\n",
            "c: 0.02488112449645996\n",
            "a: 0.002034902572631836\n",
            "b: 0.11406445503234863\n",
            "c: 0.11476802825927734\n",
            "a: 0.0020520687103271484\n",
            "b: 0.12548065185546875\n",
            "c: 0.12617802619934082\n",
            "a: 0.002038717269897461\n",
            "b: 0.17567849159240723\n",
            "c: 0.17663168907165527\n",
            "a: 0.002319812774658203\n",
            "b: 0.13973236083984375\n",
            "c: 0.1403975486755371\n",
            "a: 0.0021114349365234375\n",
            "b: 0.03685617446899414\n",
            "c: 0.03746771812438965\n",
            "a: 0.002171039581298828\n",
            "b: 0.03191089630126953\n",
            "c: 0.0323946475982666\n",
            "a: 0.0023889541625976562\n",
            "b: 0.08643865585327148\n",
            "c: 0.08711862564086914\n",
            "a: 0.0021162033081054688\n",
            "b: 0.03917717933654785\n",
            "c: 0.03981137275695801\n",
            "a: 0.0022253990173339844\n",
            "b: 0.036108970642089844\n",
            "c: 0.03682065010070801\n",
            "a: 0.002290010452270508\n",
            "b: 0.19945454597473145\n",
            "c: 0.20010089874267578\n",
            "a: 0.008443355560302734\n",
            "b: 0.05287432670593262\n",
            "c: 0.05364680290222168\n",
            "a: 0.002397298812866211\n",
            "b: 0.039362430572509766\n",
            "c: 0.04005861282348633\n",
            "a: 0.0017971992492675781\n",
            "b: 0.06204342842102051\n",
            "c: 0.06279921531677246\n",
            "a: 0.002245664596557617\n",
            "b: 0.05048418045043945\n",
            "c: 0.0506439208984375\n",
            "a: 0.0022165775299072266\n",
            "b: 0.04465365409851074\n",
            "c: 0.04490089416503906\n",
            "a: 0.002077341079711914\n",
            "b: 0.06381058692932129\n",
            "c: 0.0640401840209961\n",
            "a: 0.002222299575805664\n",
            "b: 0.05151557922363281\n",
            "c: 0.05173778533935547\n",
            "a: 0.0021500587463378906\n",
            "b: 0.06203794479370117\n",
            "c: 0.06253647804260254\n",
            "a: 0.0019516944885253906\n",
            "b: 0.05388450622558594\n",
            "c: 0.05412650108337402\n",
            "a: 0.0021295547485351562\n",
            "b: 0.06290960311889648\n",
            "c: 0.0631551742553711\n",
            "a: 0.0021486282348632812\n",
            "b: 0.05739998817443848\n",
            "c: 0.05812859535217285\n",
            "a: 0.002427339553833008\n",
            "b: 0.06682348251342773\n",
            "c: 0.06760883331298828\n",
            "a: 0.0018982887268066406\n",
            "b: 0.05958104133605957\n",
            "c: 0.060101985931396484\n",
            "a: 0.002167224884033203\n",
            "b: 0.07342648506164551\n",
            "c: 0.07411575317382812\n",
            "a: 0.0019376277923583984\n",
            "b: 0.06251072883605957\n",
            "c: 0.0631248950958252\n",
            "a: 0.0023260116577148438\n",
            "b: 0.07355856895446777\n",
            "c: 0.07423758506774902\n",
            "a: 0.0021882057189941406\n",
            "b: 0.06666254997253418\n",
            "c: 0.06731343269348145\n",
            "a: 0.0026674270629882812\n",
            "b: 0.07747030258178711\n",
            "c: 0.07769608497619629\n",
            "a: 0.0024001598358154297\n",
            "b: 0.06932568550109863\n",
            "c: 0.07001113891601562\n",
            "a: 0.002249002456665039\n",
            "b: 0.08009862899780273\n",
            "c: 0.08038187026977539\n",
            "a: 0.0029468536376953125\n",
            "b: 0.07406783103942871\n",
            "c: 0.07485032081604004\n",
            "a: 0.002191305160522461\n",
            "b: 0.07883977890014648\n",
            "c: 0.07952713966369629\n",
            "a: 0.002125263214111328\n",
            "b: 0.3563408851623535\n",
            "c: 0.35724544525146484\n",
            "a: 0.0018572807312011719\n",
            "b: 0.38540077209472656\n",
            "c: 0.3861045837402344\n",
            "a: 0.001775979995727539\n",
            "b: 0.13431572914123535\n",
            "c: 0.13506269454956055\n",
            "a: 0.0024979114532470703\n",
            "b: 0.1471843719482422\n",
            "c: 0.14806628227233887\n",
            "a: 0.0024933815002441406\n",
            "b: 0.10223507881164551\n",
            "c: 0.10276436805725098\n",
            "a: 0.0022232532501220703\n",
            "b: 0.07645130157470703\n",
            "c: 0.07708501815795898\n",
            "a: 0.0018935203552246094\n",
            "b: 0.1132211685180664\n",
            "c: 0.11400747299194336\n",
            "a: 0.0021028518676757812\n",
            "b: 0.10756492614746094\n",
            "c: 0.10781431198120117\n",
            "a: 0.0021398067474365234\n",
            "b: 0.08109140396118164\n",
            "c: 0.08176183700561523\n",
            "a: 0.0018606185913085938\n",
            "b: 0.10445690155029297\n",
            "c: 0.10525155067443848\n",
            "a: 0.002164125442504883\n",
            "b: 0.08239507675170898\n",
            "c: 0.08304858207702637\n",
            "a: 0.003246784210205078\n",
            "b: 0.108612060546875\n",
            "c: 0.1092984676361084\n",
            "a: 0.0023162364959716797\n",
            "b: 0.09051656723022461\n",
            "c: 0.09078121185302734\n",
            "a: 0.002861499786376953\n",
            "b: 0.10953044891357422\n",
            "c: 0.11019730567932129\n",
            "a: 0.0021567344665527344\n",
            "b: 0.0892031192779541\n",
            "c: 0.09027361869812012\n",
            "a: 0.0022242069244384766\n",
            "b: 0.2987940311431885\n",
            "c: 0.300034761428833\n",
            "a: 0.0022106170654296875\n",
            "b: 0.10833430290222168\n",
            "c: 0.10857415199279785\n",
            "a: 0.0018901824951171875\n",
            "b: 0.11573648452758789\n",
            "c: 0.11631965637207031\n",
            "a: 0.002286195755004883\n",
            "b: 0.10318160057067871\n",
            "c: 0.10384058952331543\n",
            "a: 0.002234935760498047\n",
            "b: 0.12801456451416016\n",
            "c: 0.1285855770111084\n",
            "a: 0.002470254898071289\n",
            "b: 0.12134766578674316\n",
            "c: 0.12164497375488281\n",
            "a: 0.002115488052368164\n",
            "b: 0.12134981155395508\n",
            "c: 0.12195611000061035\n",
            "a: 0.0019571781158447266\n",
            "b: 0.1200113296508789\n",
            "c: 0.12055730819702148\n",
            "a: 0.0018110275268554688\n",
            "b: 0.12284708023071289\n",
            "c: 0.12351751327514648\n",
            "a: 0.0020165443420410156\n",
            "b: 0.12307071685791016\n",
            "c: 0.12380385398864746\n",
            "a: 0.0019545555114746094\n",
            "b: 0.12831449508666992\n",
            "c: 0.1290738582611084\n",
            "a: 0.0017576217651367188\n",
            "b: 0.12756633758544922\n",
            "c: 0.12830281257629395\n",
            "a: 0.0019016265869140625\n",
            "b: 0.13050484657287598\n",
            "c: 0.1314074993133545\n",
            "a: 0.0019190311431884766\n",
            "b: 0.13273048400878906\n",
            "c: 0.1334846019744873\n",
            "a: 0.001905202865600586\n",
            "b: 0.1339702606201172\n",
            "c: 0.1348731517791748\n",
            "a: 0.0018935203552246094\n",
            "b: 0.13222002983093262\n",
            "c: 0.13244175910949707\n",
            "a: 0.001802206039428711\n",
            "b: 0.1346738338470459\n",
            "c: 0.13547492027282715\n",
            "a: 0.0021135807037353516\n",
            "b: 0.15757155418395996\n",
            "c: 0.15829133987426758\n",
            "a: 0.0018248558044433594\n",
            "b: 0.1801927089691162\n",
            "c: 0.18045401573181152\n",
            "a: 0.0017690658569335938\n",
            "b: 0.1381211280822754\n",
            "c: 0.13869309425354004\n",
            "a: 0.0018241405487060547\n",
            "b: 0.1478137969970703\n",
            "c: 0.14873647689819336\n",
            "a: 0.0023369789123535156\n",
            "b: 0.13990068435668945\n",
            "c: 0.1401379108428955\n",
            "a: 0.001959085464477539\n",
            "b: 0.14574885368347168\n",
            "c: 0.1464982032775879\n",
            "a: 0.0018324851989746094\n",
            "b: 0.14409232139587402\n",
            "c: 0.14471745491027832\n",
            "a: 0.0019292831420898438\n",
            "b: 0.14813494682312012\n",
            "c: 0.14904165267944336\n",
            "a: 0.002215862274169922\n",
            "b: 0.14524173736572266\n",
            "c: 0.14587950706481934\n",
            "a: 0.0018622875213623047\n",
            "b: 0.14920306205749512\n",
            "c: 0.14943552017211914\n",
            "a: 0.0020499229431152344\n",
            "b: 0.1514747142791748\n",
            "c: 0.15215563774108887\n",
            "a: 0.002189159393310547\n",
            "b: 0.15313053131103516\n",
            "c: 0.15390682220458984\n",
            "a: 0.001865386962890625\n",
            "b: 0.14935588836669922\n",
            "c: 0.15009379386901855\n",
            "a: 0.0020265579223632812\n",
            "b: 0.1582775115966797\n",
            "c: 0.15914487838745117\n",
            "a: 0.002643585205078125\n",
            "b: 0.154740571975708\n",
            "c: 0.15538597106933594\n",
            "a: 0.0021834373474121094\n",
            "b: 0.15941524505615234\n",
            "c: 0.16028451919555664\n",
            "a: 0.0018188953399658203\n",
            "b: 0.14562201499938965\n",
            "c: 0.14632272720336914\n",
            "a: 0.0019290447235107422\n",
            "b: 0.16712069511413574\n",
            "c: 0.16805171966552734\n",
            "a: 0.0018095970153808594\n",
            "b: 0.14851999282836914\n",
            "c: 0.1492764949798584\n",
            "a: 0.0024127960205078125\n",
            "b: 0.16776776313781738\n",
            "c: 0.16859102249145508\n",
            "a: 0.0022962093353271484\n",
            "b: 0.15491628646850586\n",
            "c: 0.1555643081665039\n",
            "a: 0.0018877983093261719\n",
            "b: 0.1685504913330078\n",
            "c: 0.1692523956298828\n",
            "a: 0.002164602279663086\n",
            "b: 0.15442276000976562\n",
            "c: 0.15501618385314941\n",
            "a: 0.002232074737548828\n",
            "b: 0.17502951622009277\n",
            "c: 0.17592763900756836\n",
            "a: 0.002050161361694336\n",
            "b: 0.15637683868408203\n",
            "c: 0.15702271461486816\n",
            "a: 0.0018641948699951172\n",
            "b: 0.17387080192565918\n",
            "c: 0.17463231086730957\n",
            "a: 0.0019378662109375\n",
            "b: 0.15851306915283203\n",
            "c: 0.15916919708251953\n",
            "a: 0.0021867752075195312\n",
            "b: 0.1756289005279541\n",
            "c: 0.1764392852783203\n",
            "a: 0.0021986961364746094\n",
            "b: 0.8787577152252197\n",
            "c: 0.8790204524993896\n",
            "a: 0.0024394989013671875\n",
            "b: 0.7161228656768799\n",
            "c: 0.7172980308532715\n",
            "a: 0.00312042236328125\n",
            "b: 0.5473721027374268\n",
            "c: 0.5480399131774902\n",
            "a: 0.002434253692626953\n",
            "b: 0.16864871978759766\n",
            "c: 0.16888022422790527\n",
            "a: 0.0024378299713134766\n",
            "b: 0.22223281860351562\n",
            "c: 0.22250676155090332\n",
            "a: 0.0018246173858642578\n",
            "b: 0.18413233757019043\n",
            "c: 0.184906005859375\n",
            "a: 0.002240419387817383\n",
            "b: 0.1626138687133789\n",
            "c: 0.16354680061340332\n",
            "a: 0.0022983551025390625\n",
            "b: 0.224076509475708\n",
            "c: 0.22483181953430176\n",
            "a: 0.002227306365966797\n",
            "b: 0.18878173828125\n",
            "c: 0.1894547939300537\n",
            "a: 0.0020835399627685547\n",
            "b: 0.16820311546325684\n",
            "c: 0.16904139518737793\n",
            "a: 0.0020036697387695312\n",
            "b: 0.2287003993988037\n",
            "c: 0.2294313907623291\n",
            "a: 0.001856088638305664\n",
            "b: 0.19339919090270996\n",
            "c: 0.19403934478759766\n",
            "a: 0.0019068717956542969\n",
            "b: 0.16894793510437012\n",
            "c: 0.16969037055969238\n",
            "a: 0.001912832260131836\n",
            "b: 0.23337173461914062\n",
            "c: 0.2342700958251953\n",
            "a: 0.0019352436065673828\n",
            "b: 0.20133161544799805\n",
            "c: 0.2017498016357422\n",
            "a: 0.0030946731567382812\n",
            "b: 0.17441534996032715\n",
            "c: 0.17509794235229492\n",
            "a: 0.0020160675048828125\n",
            "b: 0.23865103721618652\n",
            "c: 0.23941397666931152\n",
            "a: 0.0017554759979248047\n",
            "b: 0.20392608642578125\n",
            "c: 0.2043769359588623\n",
            "a: 0.0031070709228515625\n",
            "b: 0.1789393424987793\n",
            "c: 0.1795191764831543\n",
            "a: 0.0018265247344970703\n",
            "b: 0.24853825569152832\n",
            "c: 0.24928498268127441\n",
            "a: 0.002202272415161133\n",
            "b: 0.33499598503112793\n",
            "c: 0.3359055519104004\n",
            "a: 0.002073526382446289\n",
            "b: 0.26720166206359863\n",
            "c: 0.2679867744445801\n",
            "a: 0.0020165443420410156\n",
            "b: 0.4393129348754883\n",
            "c: 0.4402191638946533\n",
            "a: 0.001851797103881836\n",
            "b: 0.613922119140625\n",
            "c: 0.6145799160003662\n",
            "a: 0.0020825862884521484\n",
            "b: 0.3257629871368408\n",
            "c: 0.32664060592651367\n",
            "a: 0.002198934555053711\n",
            "b: 0.5529766082763672\n",
            "c: 0.5537517070770264\n",
            "a: 0.0018639564514160156\n",
            "b: 0.5114212036132812\n",
            "c: 0.5123484134674072\n",
            "a: 0.002264261245727539\n",
            "b: 0.29104113578796387\n",
            "c: 0.29184913635253906\n",
            "a: 0.0018663406372070312\n",
            "b: 0.3073389530181885\n",
            "c: 0.30797314643859863\n",
            "a: 0.0019502639770507812\n",
            "b: 0.25948071479797363\n",
            "c: 0.26035547256469727\n",
            "a: 0.0018126964569091797\n",
            "b: 0.20201349258422852\n",
            "c: 0.2022418975830078\n",
            "a: 0.0025489330291748047\n",
            "b: 0.26354241371154785\n",
            "c: 0.26435422897338867\n",
            "a: 0.0019145011901855469\n",
            "b: 0.5478360652923584\n",
            "c: 0.5487511157989502\n",
            "a: 0.002162456512451172\n",
            "b: 0.21317696571350098\n",
            "c: 0.21408414840698242\n",
            "a: 0.0023071765899658203\n",
            "b: 0.26653528213500977\n",
            "c: 0.26724863052368164\n",
            "a: 0.002210378646850586\n",
            "b: 0.2337329387664795\n",
            "c: 0.23399615287780762\n",
            "a: 0.00261688232421875\n",
            "b: 0.20324349403381348\n",
            "c: 0.20366787910461426\n",
            "a: 0.003127574920654297\n",
            "b: 0.8890664577484131\n",
            "c: 0.8898525238037109\n",
            "a: 0.0018663406372070312\n",
            "b: 0.3500328063964844\n",
            "c: 0.350827693939209\n",
            "a: 0.0018856525421142578\n",
            "b: 0.7755649089813232\n",
            "c: 0.7763724327087402\n",
            "a: 0.0021140575408935547\n",
            "b: 0.40834641456604004\n",
            "c: 0.40931224822998047\n",
            "a: 0.0019197463989257812\n",
            "b: 0.7126731872558594\n",
            "c: 0.7138197422027588\n",
            "a: 0.0018930435180664062\n",
            "b: 0.43389463424682617\n",
            "c: 0.43482398986816406\n",
            "a: 0.0019953250885009766\n",
            "b: 0.7611570358276367\n",
            "c: 0.761988639831543\n",
            "a: 0.001994609832763672\n",
            "b: 0.2315354347229004\n",
            "c: 0.23244190216064453\n",
            "a: 0.0024404525756835938\n",
            "b: 1.053527593612671\n",
            "c: 1.0544772148132324\n",
            "a: 0.002193927764892578\n",
            "b: 0.38677453994750977\n",
            "c: 0.387617826461792\n",
            "a: 0.002088308334350586\n",
            "b: 0.8557000160217285\n",
            "c: 0.856564998626709\n",
            "a: 0.0018627643585205078\n",
            "b: 0.41579174995422363\n",
            "c: 0.4167001247406006\n",
            "a: 0.0020542144775390625\n",
            "b: 0.7471082210540771\n",
            "c: 0.7478113174438477\n",
            "a: 0.0019750595092773438\n",
            "b: 0.4555342197418213\n",
            "c: 0.45636868476867676\n",
            "a: 0.0024564266204833984\n",
            "b: 0.7837004661560059\n",
            "c: 0.784616231918335\n",
            "a: 0.0019207000732421875\n",
            "b: 0.2553131580352783\n",
            "c: 0.25610995292663574\n",
            "a: 0.001856088638305664\n",
            "b: 1.0666611194610596\n",
            "c: 1.0674328804016113\n",
            "a: 0.0020380020141601562\n",
            "b: 0.4413433074951172\n",
            "c: 0.4420187473297119\n",
            "a: 0.0020437240600585938\n",
            "b: 0.8557438850402832\n",
            "c: 0.8566176891326904\n",
            "a: 0.0021424293518066406\n",
            "b: 0.41914939880371094\n",
            "c: 0.42002177238464355\n",
            "a: 0.0023136138916015625\n",
            "b: 0.8089330196380615\n",
            "c: 0.80971360206604\n",
            "a: 0.001954317092895508\n",
            "b: 0.4854869842529297\n",
            "c: 0.4864339828491211\n",
            "a: 0.002010822296142578\n",
            "b: 0.7856497764587402\n",
            "c: 0.7864797115325928\n",
            "a: 0.0018072128295898438\n",
            "b: 0.34651613235473633\n",
            "c: 0.34744906425476074\n",
            "a: 0.002131938934326172\n",
            "b: 1.238722562789917\n",
            "c: 1.2395663261413574\n",
            "a: 0.0021584033966064453\n",
            "b: 0.46064019203186035\n",
            "c: 0.4613642692565918\n",
            "a: 0.0018696784973144531\n",
            "b: 0.9167509078979492\n",
            "c: 0.9176149368286133\n",
            "a: 0.0019910335540771484\n",
            "b: 0.3073432445526123\n",
            "c: 0.3081653118133545\n",
            "a: 0.0015645027160644531\n",
            "b: 0.8351759910583496\n",
            "c: 0.8360235691070557\n",
            "a: 0.0021004676818847656\n",
            "b: 0.5476794242858887\n",
            "c: 0.5483100414276123\n",
            "a: 0.0018482208251953125\n",
            "b: 0.859959602355957\n",
            "c: 0.8602323532104492\n",
            "a: 0.002083301544189453\n",
            "b: 0.31811046600341797\n",
            "c: 0.31839609146118164\n",
            "a: 0.0019807815551757812\n",
            "b: 1.0558812618255615\n",
            "c: 1.0566234588623047\n",
            "a: 0.0020494461059570312\n",
            "b: 0.6722726821899414\n",
            "c: 0.6731703281402588\n",
            "a: 0.0021886825561523438\n",
            "b: 0.8813731670379639\n",
            "c: 0.8820648193359375\n",
            "a: 0.002387523651123047\n",
            "b: 0.3379793167114258\n",
            "c: 0.3387720584869385\n",
            "a: 0.0020325183868408203\n",
            "b: 0.7708299160003662\n",
            "c: 0.771481990814209\n",
            "a: 0.002061128616333008\n",
            "b: 0.7444441318511963\n",
            "c: 0.7452595233917236\n",
            "a: 0.0019867420196533203\n",
            "b: 0.8745744228363037\n",
            "c: 0.875237226486206\n",
            "a: 0.0026221275329589844\n",
            "b: 0.28307461738586426\n",
            "c: 0.2838287353515625\n",
            "a: 0.0018825531005859375\n",
            "b: 1.0407614707946777\n",
            "c: 1.041020154953003\n",
            "a: 0.0019044876098632812\n",
            "b: 0.7726151943206787\n",
            "c: 0.7734510898590088\n",
            "a: 0.0018584728240966797\n",
            "b: 0.8947021961212158\n",
            "c: 0.8956084251403809\n",
            "a: 0.0020995140075683594\n",
            "b: 0.4311206340789795\n",
            "c: 0.4320707321166992\n",
            "a: 0.0019366741180419922\n",
            "b: 0.7855701446533203\n",
            "c: 0.7863631248474121\n",
            "a: 0.0018830299377441406\n",
            "b: 0.8013606071472168\n",
            "c: 0.8022511005401611\n",
            "a: 0.0019555091857910156\n",
            "b: 0.8990321159362793\n",
            "c: 0.8997578620910645\n",
            "a: 0.0023458003997802734\n",
            "b: 0.40662217140197754\n",
            "c: 0.40755271911621094\n",
            "a: 0.0026171207427978516\n",
            "b: 1.0415220260620117\n",
            "c: 1.041795015335083\n",
            "a: 0.0023796558380126953\n",
            "b: 0.8659720420837402\n",
            "c: 0.8667354583740234\n",
            "a: 0.0024695396423339844\n",
            "b: 0.9882242679595947\n",
            "c: 0.988497257232666\n",
            "a: 0.0028533935546875\n",
            "b: 0.629035234451294\n",
            "c: 0.6298387050628662\n",
            "a: 0.0024492740631103516\n",
            "b: 0.9759702682495117\n",
            "c: 0.9762048721313477\n",
            "a: 0.0024521350860595703\n",
            "b: 0.9225976467132568\n",
            "c: 0.923382043838501\n",
            "a: 0.002479076385498047\n",
            "b: 0.9998180866241455\n",
            "c: 1.000643014907837\n",
            "a: 0.0027494430541992188\n",
            "b: 0.6826064586639404\n",
            "c: 0.6828384399414062\n",
            "a: 0.0026397705078125\n",
            "b: 1.055809497833252\n",
            "c: 1.0560457706451416\n",
            "a: 0.002607583999633789\n",
            "b: 0.998239278793335\n",
            "c: 0.9990108013153076\n",
            "a: 0.0022444725036621094\n",
            "b: 1.2246043682098389\n",
            "c: 1.2253928184509277\n",
            "a: 0.0026013851165771484\n",
            "b: 0.74434494972229\n",
            "c: 0.7452163696289062\n",
            "a: 0.002258777618408203\n",
            "b: 1.0575170516967773\n",
            "c: 1.0583252906799316\n",
            "a: 0.002346515655517578\n",
            "b: 1.0075368881225586\n",
            "c: 1.0083394050598145\n",
            "a: 0.0022923946380615234\n",
            "b: 1.228578805923462\n",
            "c: 1.22922682762146\n",
            "a: 0.0025801658630371094\n",
            "b: 0.8878822326660156\n",
            "c: 0.8887042999267578\n",
            "a: 0.0022852420806884766\n",
            "b: 1.0733180046081543\n",
            "c: 1.0735974311828613\n",
            "a: 0.0027551651000976562\n",
            "b: 1.0579442977905273\n",
            "c: 1.0586493015289307\n",
            "a: 0.002441883087158203\n",
            "b: 1.250246286392212\n",
            "c: 1.251082420349121\n",
            "a: 0.0027303695678710938\n",
            "b: 1.0242867469787598\n",
            "c: 1.0249760150909424\n",
            "a: 0.002178668975830078\n",
            "b: 1.0659685134887695\n",
            "c: 1.066781997680664\n",
            "a: 0.002379894256591797\n",
            "b: 1.1121399402618408\n",
            "c: 1.1124458312988281\n",
            "a: 0.0025255680084228516\n",
            "b: 1.2844858169555664\n",
            "c: 1.2852888107299805\n",
            "a: 0.032352447509765625\n",
            "b: 0.7487654685974121\n",
            "c: 0.7494819164276123\n",
            "a: 0.0022399425506591797\n",
            "b: 0.45598649978637695\n",
            "c: 0.4568295478820801\n",
            "a: 0.0022461414337158203\n",
            "b: 0.3273801803588867\n",
            "c: 0.32813024520874023\n",
            "a: 0.002119302749633789\n",
            "b: 0.3641386032104492\n",
            "c: 0.3643991947174072\n",
            "a: 0.002233266830444336\n",
            "b: 0.32997870445251465\n",
            "c: 0.33077287673950195\n",
            "a: 0.002304553985595703\n",
            "b: 0.36819958686828613\n",
            "c: 0.3689768314361572\n",
            "a: 0.0022509098052978516\n",
            "b: 0.33309245109558105\n",
            "c: 0.33380842208862305\n",
            "a: 0.0022881031036376953\n",
            "b: 0.371112585067749\n",
            "c: 0.3719198703765869\n",
            "a: 0.0024106502532958984\n",
            "b: 0.336899995803833\n",
            "c: 0.3375976085662842\n",
            "a: 0.00186920166015625\n",
            "b: 0.3622167110443115\n",
            "c: 0.3630342483520508\n",
            "a: 0.002288818359375\n",
            "b: 0.33952975273132324\n",
            "c: 0.3403627872467041\n",
            "a: 0.002141237258911133\n",
            "b: 0.36764025688171387\n",
            "c: 0.3685481548309326\n",
            "a: 0.0020689964294433594\n",
            "b: 0.3400845527648926\n",
            "c: 0.3409421443939209\n",
            "a: 0.0022401809692382812\n",
            "b: 0.36650800704956055\n",
            "c: 0.36733055114746094\n",
            "a: 0.001951456069946289\n",
            "b: 0.344465970993042\n",
            "c: 0.3452897071838379\n",
            "a: 0.0023076534271240234\n",
            "b: 0.37472057342529297\n",
            "c: 0.37574052810668945\n",
            "a: 0.002289295196533203\n",
            "b: 0.3519253730773926\n",
            "c: 0.35272884368896484\n",
            "a: 0.002205371856689453\n",
            "b: 0.38240718841552734\n",
            "c: 0.38269662857055664\n",
            "a: 0.0022690296173095703\n",
            "b: 0.35040855407714844\n",
            "c: 0.3511929512023926\n",
            "a: 0.0023186206817626953\n",
            "b: 0.38773441314697266\n",
            "c: 0.3885304927825928\n",
            "a: 0.002199411392211914\n",
            "b: 0.34801363945007324\n",
            "c: 0.3482553958892822\n",
            "a: 0.002286672592163086\n",
            "b: 0.39192843437194824\n",
            "c: 0.39260268211364746\n",
            "a: 0.0022487640380859375\n",
            "b: 0.34561991691589355\n",
            "c: 0.34629321098327637\n",
            "a: 0.0018610954284667969\n",
            "b: 0.39224696159362793\n",
            "c: 0.39312076568603516\n",
            "a: 0.002290010452270508\n",
            "b: 0.3456711769104004\n",
            "c: 0.3459510803222656\n",
            "a: 0.0020639896392822266\n",
            "b: 0.3952362537384033\n",
            "c: 0.3954923152923584\n",
            "a: 0.0021119117736816406\n",
            "b: 0.348142147064209\n",
            "c: 0.3489694595336914\n",
            "a: 0.0019664764404296875\n",
            "b: 0.3946652412414551\n",
            "c: 0.39557814598083496\n",
            "a: 0.001956939697265625\n",
            "b: 0.352459192276001\n",
            "c: 0.3533508777618408\n",
            "a: 0.002246856689453125\n",
            "b: 0.39887475967407227\n",
            "c: 0.3998596668243408\n",
            "a: 0.0021944046020507812\n",
            "b: 0.3557400703430176\n",
            "c: 0.35636329650878906\n",
            "a: 0.002458810806274414\n",
            "b: 0.40143847465515137\n",
            "c: 0.40220212936401367\n",
            "a: 0.001998424530029297\n",
            "b: 0.3578026294708252\n",
            "c: 0.3580598831176758\n",
            "a: 0.0022363662719726562\n",
            "b: 0.40723609924316406\n",
            "c: 0.4080779552459717\n",
            "a: 0.0023255348205566406\n",
            "b: 0.36106014251708984\n",
            "c: 0.36185789108276367\n",
            "a: 0.002319812774658203\n",
            "b: 0.4024791717529297\n",
            "c: 0.40274834632873535\n",
            "a: 0.0026581287384033203\n",
            "b: 0.371523380279541\n",
            "c: 0.37230515480041504\n",
            "a: 0.002019643783569336\n",
            "b: 0.4108450412750244\n",
            "c: 0.41169214248657227\n",
            "a: 0.0025701522827148438\n",
            "b: 0.3695831298828125\n",
            "c: 0.3703749179840088\n",
            "a: 0.0022296905517578125\n",
            "b: 0.41675281524658203\n",
            "c: 0.4175748825073242\n",
            "a: 0.0022826194763183594\n",
            "b: 0.36890673637390137\n",
            "c: 0.3697676658630371\n",
            "a: 0.0019369125366210938\n",
            "b: 0.4197714328765869\n",
            "c: 0.42041540145874023\n",
            "a: 0.0018768310546875\n",
            "b: 0.3727443218231201\n",
            "c: 0.3735330104827881\n",
            "a: 0.0021708011627197266\n",
            "b: 0.4265303611755371\n",
            "c: 0.4272580146789551\n",
            "a: 0.002323150634765625\n",
            "b: 0.3761627674102783\n",
            "c: 0.376415491104126\n",
            "a: 0.00283050537109375\n",
            "b: 0.4205787181854248\n",
            "c: 0.4212040901184082\n",
            "a: 0.0021135807037353516\n",
            "b: 0.3811454772949219\n",
            "c: 0.38144850730895996\n",
            "a: 0.002047300338745117\n",
            "b: 0.42468905448913574\n",
            "c: 0.42534732818603516\n",
            "a: 0.0019295215606689453\n",
            "b: 0.37969303131103516\n",
            "c: 0.380537748336792\n",
            "a: 0.0019044876098632812\n",
            "b: 0.42163515090942383\n",
            "c: 0.42243099212646484\n",
            "a: 0.0018544197082519531\n",
            "b: 0.3806648254394531\n",
            "c: 0.38153767585754395\n",
            "a: 0.0017895698547363281\n",
            "b: 0.4263174533843994\n",
            "c: 0.4270167350769043\n",
            "a: 0.0020148754119873047\n",
            "b: 0.3926682472229004\n",
            "c: 0.39291834831237793\n",
            "a: 0.001968860626220703\n",
            "b: 0.427717924118042\n",
            "c: 0.4283256530761719\n",
            "a: 0.0019998550415039062\n",
            "b: 0.39218902587890625\n",
            "c: 0.39302682876586914\n",
            "a: 0.0022268295288085938\n",
            "b: 0.4228634834289551\n",
            "c: 0.4235188961029053\n",
            "a: 0.0019578933715820312\n",
            "b: 0.3925602436065674\n",
            "c: 0.3933556079864502\n",
            "a: 0.0019659996032714844\n",
            "b: 0.4344451427459717\n",
            "c: 0.43523454666137695\n",
            "a: 0.002299785614013672\n",
            "b: 0.39841580390930176\n",
            "c: 0.39928412437438965\n",
            "a: 0.0105133056640625\n",
            "b: 0.45670247077941895\n",
            "c: 0.4574856758117676\n",
            "a: 0.0019936561584472656\n",
            "b: 0.4001493453979492\n",
            "c: 0.4009554386138916\n",
            "a: 0.002245187759399414\n",
            "b: 0.4405341148376465\n",
            "c: 0.441331148147583\n",
            "a: 0.001958131790161133\n",
            "b: 0.397946834564209\n",
            "c: 0.39879512786865234\n",
            "a: 0.0019228458404541016\n",
            "b: 0.4469461441040039\n",
            "c: 0.4477410316467285\n",
            "a: 0.002168893814086914\n",
            "b: 0.40216517448425293\n",
            "c: 0.40290403366088867\n",
            "a: 0.002084016799926758\n",
            "b: 0.4489312171936035\n",
            "c: 0.44958996772766113\n",
            "a: 0.0019283294677734375\n",
            "b: 0.4085197448730469\n",
            "c: 0.40938878059387207\n",
            "a: 0.0020723342895507812\n",
            "b: 0.44125986099243164\n",
            "c: 0.44188690185546875\n",
            "a: 0.0018143653869628906\n",
            "b: 0.408829927444458\n",
            "c: 0.4091053009033203\n",
            "a: 0.001996278762817383\n",
            "b: 0.4417426586151123\n",
            "c: 0.4420187473297119\n",
            "a: 0.0030236244201660156\n",
            "b: 0.4069366455078125\n",
            "c: 0.407825231552124\n",
            "a: 0.0019915103912353516\n",
            "b: 0.46062278747558594\n",
            "c: 0.46149492263793945\n",
            "a: 0.00231170654296875\n",
            "b: 0.41159725189208984\n",
            "c: 0.4124143123626709\n",
            "a: 0.002244710922241211\n",
            "b: 0.46666431427001953\n",
            "c: 0.46692419052124023\n",
            "a: 0.002203702926635742\n",
            "b: 0.41579580307006836\n",
            "c: 0.41669702529907227\n",
            "a: 0.001950979232788086\n",
            "b: 0.4612400531768799\n",
            "c: 0.4619615077972412\n",
            "a: 0.002031564712524414\n",
            "b: 0.41835761070251465\n",
            "c: 0.41901063919067383\n",
            "a: 0.0020334720611572266\n",
            "b: 0.4599263668060303\n",
            "c: 0.4606297016143799\n",
            "a: 0.0023064613342285156\n",
            "b: 0.4227275848388672\n",
            "c: 0.42336130142211914\n",
            "a: 0.002009153366088867\n",
            "b: 0.4679560661315918\n",
            "c: 0.46860599517822266\n",
            "a: 0.0021469593048095703\n",
            "b: 0.4278280735015869\n",
            "c: 0.4285423755645752\n",
            "a: 0.002165079116821289\n",
            "b: 0.4717674255371094\n",
            "c: 0.47203803062438965\n",
            "a: 0.0022444725036621094\n",
            "b: 0.42940354347229004\n",
            "c: 0.430295467376709\n",
            "a: 0.0018808841705322266\n",
            "b: 0.4737386703491211\n",
            "c: 0.4744594097137451\n",
            "a: 0.001982450485229492\n",
            "b: 0.4279897212982178\n",
            "c: 0.4282402992248535\n",
            "a: 0.0019614696502685547\n",
            "b: 0.6022298336029053\n",
            "c: 0.6025331020355225\n",
            "a: 0.0020990371704101562\n",
            "b: 0.6137495040893555\n",
            "c: 0.6145710945129395\n",
            "a: 0.0018553733825683594\n",
            "b: 0.5580699443817139\n",
            "c: 0.558880090713501\n",
            "a: 0.0021588802337646484\n",
            "b: 0.4482898712158203\n",
            "c: 0.4491844177246094\n",
            "a: 0.002062082290649414\n",
            "b: 0.4913146495819092\n",
            "c: 0.4921550750732422\n",
            "a: 0.0017924308776855469\n",
            "b: 0.45158958435058594\n",
            "c: 0.4522268772125244\n",
            "a: 0.0021157264709472656\n",
            "b: 0.5004315376281738\n",
            "c: 0.501093864440918\n",
            "a: 0.003065824508666992\n",
            "b: 0.46461939811706543\n",
            "c: 0.46544384956359863\n",
            "a: 0.002182483673095703\n",
            "b: 0.48798370361328125\n",
            "c: 0.4882621765136719\n",
            "a: 0.0019724369049072266\n",
            "b: 0.44739747047424316\n",
            "c: 0.44829583168029785\n",
            "a: 0.0018012523651123047\n",
            "b: 0.5070195198059082\n",
            "c: 0.5078613758087158\n",
            "a: 0.0042116641998291016\n",
            "b: 0.4728090763092041\n",
            "c: 0.4735300540924072\n",
            "a: 0.0022602081298828125\n",
            "b: 0.6325428485870361\n",
            "c: 0.6333613395690918\n",
            "a: 0.002796649932861328\n",
            "b: 0.6537704467773438\n",
            "c: 0.6544151306152344\n",
            "a: 0.0022039413452148438\n",
            "b: 0.6540343761444092\n",
            "c: 0.6542987823486328\n",
            "a: 0.002389192581176758\n",
            "b: 0.48829030990600586\n",
            "c: 0.48914027214050293\n",
            "a: 0.001875162124633789\n",
            "b: 0.5097236633300781\n",
            "c: 0.5105760097503662\n",
            "a: 0.0023415088653564453\n",
            "b: 0.47414350509643555\n",
            "c: 0.47484731674194336\n",
            "a: 0.0020995140075683594\n",
            "b: 0.5037288665771484\n",
            "c: 0.5044631958007812\n",
            "a: 0.00228118896484375\n",
            "b: 0.46807050704956055\n",
            "c: 0.4689781665802002\n",
            "a: 0.002170085906982422\n",
            "b: 0.508554220199585\n",
            "c: 0.5092871189117432\n",
            "a: 0.0019176006317138672\n",
            "b: 0.4752964973449707\n",
            "c: 0.4760105609893799\n",
            "a: 0.0021827220916748047\n",
            "b: 0.5101737976074219\n",
            "c: 0.5104420185089111\n",
            "a: 0.002029895782470703\n",
            "b: 0.4727602005004883\n",
            "c: 0.4736344814300537\n",
            "a: 0.002111196517944336\n",
            "b: 0.5137474536895752\n",
            "c: 0.5139970779418945\n",
            "a: 0.0021059513092041016\n",
            "b: 0.47557640075683594\n",
            "c: 0.4758615493774414\n",
            "a: 0.002722501754760742\n",
            "b: 0.5169868469238281\n",
            "c: 0.5172343254089355\n",
            "a: 0.0020782947540283203\n",
            "b: 0.4838387966156006\n",
            "c: 0.48410797119140625\n",
            "a: 0.002649068832397461\n",
            "b: 0.5174596309661865\n",
            "c: 0.5177147388458252\n",
            "a: 0.001953601837158203\n",
            "b: 0.4752922058105469\n",
            "c: 0.47612786293029785\n",
            "a: 0.002188444137573242\n",
            "b: 0.5546174049377441\n",
            "c: 0.5555307865142822\n",
            "a: 0.002240419387817383\n",
            "b: 0.49274778366088867\n",
            "c: 0.49302077293395996\n",
            "a: 0.0026171207427978516\n",
            "b: 0.5280909538269043\n",
            "c: 0.5288686752319336\n",
            "a: 0.0018811225891113281\n",
            "b: 0.494748592376709\n",
            "c: 0.4950699806213379\n",
            "a: 0.0027265548706054688\n",
            "b: 0.5335683822631836\n",
            "c: 0.53438401222229\n",
            "a: 0.0018575191497802734\n",
            "b: 0.4928722381591797\n",
            "c: 0.4931447505950928\n",
            "a: 0.002765655517578125\n",
            "b: 0.5325319766998291\n",
            "c: 0.5331783294677734\n",
            "a: 0.0018537044525146484\n",
            "b: 0.4936399459838867\n",
            "c: 0.49390745162963867\n",
            "a: 0.002006053924560547\n",
            "b: 0.5364093780517578\n",
            "c: 0.5366470813751221\n",
            "a: 0.002353668212890625\n",
            "b: 0.49988532066345215\n",
            "c: 0.5001821517944336\n",
            "a: 0.0023458003997802734\n",
            "b: 0.5889809131622314\n",
            "c: 0.5899255275726318\n",
            "a: 0.0021064281463623047\n",
            "b: 0.5669686794281006\n",
            "c: 0.5678665637969971\n",
            "a: 0.0017228126525878906\n",
            "b: 0.8006515502929688\n",
            "c: 0.806957483291626\n",
            "a: 0.002858877182006836\n",
            "b: 0.6976096630096436\n",
            "c: 0.6983306407928467\n",
            "a: 0.00747990608215332\n",
            "b: 0.7089114189147949\n",
            "c: 0.7096900939941406\n",
            "a: 0.0026085376739501953\n",
            "b: 1.227625846862793\n",
            "c: 1.2284111976623535\n",
            "t-shirt_1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-84-deaf68dd7368>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;31m# Train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0madam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     model.compile(loss='categorical_crossentropy',\n\u001b[1;32m     41\u001b[0m                   \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madam\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow._api.v2.train' has no attribute 'AdamOptimizer'"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPoAAAD4CAYAAADFJPs2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\noKCLOKCgizigoIs48P+XobNglD7UBQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "_YRSRkOyBP1P",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Training "
      ]
    },
    {
      "metadata": {
        "id": "7OMEJ7kF3lsP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.fit(x = x_train, y = y_train, validation_split=0.1, batch_size = 256, verbose=2, epochs=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d2KztY7qEn9_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Testing "
      ]
    },
    {
      "metadata": {
        "id": "ssaZczS7DxeA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test accuarcy: {:0.2f}%'.format(score[1] * 100))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9xBM_w0VBbNr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Inference "
      ]
    },
    {
      "metadata": {
        "id": "nH3JfoiYHdpk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from random import randint\n",
        "%matplotlib inline  \n",
        "idx = randint(0, len(x_test))\n",
        "img = x_test[idx]\n",
        "plt.imshow(img.squeeze()) \n",
        "pred = model.predict(np.expand_dims(img, axis=0))[0]\n",
        "ind = (-pred).argsort()[:5]\n",
        "latex = [class_names[x] for x in ind]\n",
        "print(latex)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YPp5D82YBhM-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Store the classes "
      ]
    },
    {
      "metadata": {
        "id": "NoFI1msFYpCN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with open('class_names.txt', 'w') as file_handler:\n",
        "    for item in class_names:\n",
        "        file_handler.write(\"{}\\n\".format(item))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mfJ6dpaDBpRx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Install TensorFlowJS"
      ]
    },
    {
      "metadata": {
        "id": "hJJDfp9mY9Xh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install tensorflowjs "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-oBl0ZKVB00d",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Save and Convert "
      ]
    },
    {
      "metadata": {
        "id": "XVICB3TbZGb2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.save('keras.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bTWWlGdWZOvs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mkdir model\n",
        "!tensorflowjs_converter --input_format keras keras.h5 model/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JKYxE2MEB6LV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Zip and Download "
      ]
    },
    {
      "metadata": {
        "id": "865-t79uaB63",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!cp class_names.txt model/class_names.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GLC-MzW8ZXTa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!zip -r model.zip model "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4vfPR03xZZeD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('model.zip')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}